# 05 Validity and Blind Spots

## Construct Validity
- Does metric really measure policy following?
  - Pass@1 requires simultaneously reaching the ground-truth state S* and satisfying the hidden LTL oracles (Section 5.1), so it directly penalizes unsafe successes, but it still assumes that the provided regulation excerpts were read and grounded—the evaluation cannot tell whether a model inferred the safety rule from context or merely memorized common safety scaffolding.

## Internal Validity
- Potential confounders/leakage:
  - Safety masking may change instruction difficulty unevenly between goal vs workflow prompts, so drops in Figure 2 could reflect prompt style rather than implicit compliance ability (Section 3.4).
  - GPT-5-Mini performs both policy extraction and masking; if it leaks stylistic artifacts, other GPT-5 variants might gain unfair familiarity relative to non-OpenAI models.
  - Human verification (73.9% oracle acceptance, 70.6% trace acceptance) lacks inter-rater metrics (Section 4.2), so residual errors in Φ or τ* could mislabel agent behavior.

## External Validity
- Generalization limits (tasks/models/policies):
  - Only three domains with specific regulations (PSD2, HIPAA Security Rule, ETSI EN 303 645) and ToolEmu-derived APIs were considered (Section 4.1); agents facing different statutes, jurisdictions, languages, or actuation modalities might behave differently.
  - All tasks require Python tool invocation within a sandbox; multimodal reasoning, natural-language-only assistants, or long-horizon real-world actuators remain untested.
  - Evaluation uses 240 tasks, so statistical power for per-model per-domain claims is limited once split across goal vs workflow instructions.

## Robustness Coverage
- Adversarial prompts: Not covered—user instructions remain benign goal/workflow descriptions generated by the pipeline (Section 3.4).
- Ambiguous/conflicting policies: Regulations are distilled into deterministic LTL templates and deduplicated, so conflicting/ambiguous clauses are intentionally excluded (Section 3.2), leaving that stressor untested.
- Long-horizon behavior: Fuzzer-generated traces explore bounded-depth API sequences, but horizon length distributions are not reported; complex multi-dozen-step workflows likely absent.
- Tool-use constraints: Schema preconditions are enforced during fuzzing (Section 3.3), yet there is no variation in tool availability or dynamic API changes, so models are never forced to handle missing functions or degraded services.

## Reproducibility
- Missing implementation/eval details:
  - The paper states the fuzzer is ≈3,600 lines but does not release pseudocode, hyperparameters (depth limits, sampling heuristics), or the exact policy prompts used for GPT-5-Mini (Sections 3.3–3.4).
  - Experimental setup (Section 5.1) lacks API version numbers, temperature/decoding parameters, and evaluation compute budget, complicating replication.
  - Verification protocol (Section 4.2) omits reviewer guidelines and disagreement resolution procedures, so independent curation could diverge.

## Top Blind Spots
1. Reliance on only two LTL templates risks missing nuanced obligations (e.g., time-bounded requirements, multi-entity dependencies) present in real regulations.
2. Absence of adversarial or conflicting instructions means the benchmark may underestimate compliance failures triggered by malicious or ambiguous users.
3. Evaluation is limited to static sandboxes; there is no evidence that models would remain compliant when executing in live systems with stochastic tool responses or observation noise.
