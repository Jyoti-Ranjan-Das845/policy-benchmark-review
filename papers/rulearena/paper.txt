Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 550–572
July 27 - August 1, 2025 ©2025 Association for Computational Linguistics
RULE ARENA : A Benchmark for Rule-Guided Reasoning with LLMs in
Real-World Scenarios
Ruiwen Zhou1, Wenyue Hua 1, Liangming Pan 2, Sitao Cheng 1,
Xiaobao Wu1,3, En Yu 1, William Yang Wang 1
1University of California, Santa Barbara 2University of Arizona
3Nanyang Technological University
Abstract
This paper introduces RULE ARENA , a novel
and challenging benchmark designed to evalu-
ate the ability of large language models (LLMs)
to follow complex, real-world rules in rea-
soning. Covering three practical domains—
airline baggage fees, NBA transactions, and
tax regulations— RULE ARENA assesses LLMs’
proficiency in handling intricate natural lan-
guage instructions that demand long-context
understanding, logical reasoning, and accurate
mathematical computation. Two key attributes
distinguish RULE ARENA from traditional rule-
based reasoning benchmarks: (1) it extends
beyond standard first-order logic representa-
tions, and (2) it is grounded in authentic, prac-
tical scenarios, providing insights into the suit-
ability and reliability of LLMs for real-world
applications. Our findings reveal several no-
table limitations in LLMs: (1) they struggle to
identify and apply the appropriate rules, fre-
quently becoming confused by similar but dis-
tinct regulations, (2) they cannot consistently
perform accurate mathematical computations,
even when they correctly identify the relevant
rules, and (3) in general, they perform poorly in
the benchmark. We also observe a significant
performance boost when LLMs are provided
with external tools for oracle math and logic
operations. These results highlight significant
challenges and promising research directions
in advancing LLMs’ rule-guided reasoning ca-
pabilities in real-life applications. Our codes
and data are publicly available on GitHub.
1 Introduction
Recently, Large Language Models (LLMs) (Tou-
vron et al., 2023; OpenAI, 2023; Team, 2023; An-
thropic, 2024) have demonstrated remarkable capa-
bilities across many real-world applications. How-
ever, their limited domain-specific knowledge often
leads to unfaithful or misleading output, which can
cause significant risks and financial liabilities. For
example, Canadian airline was recently required
to compensate a customer who received incorrect
guidance from the airline’s chatbot1. These chal-
lenges highlight the need for robust, real-world
benchmarks that assess how faithfully and accu-
rately LLMs can follow real-life instructions and
adhere to relevant regulations, thereby ensuring
reliable and safe outputs for deployment.
Although several studies have examined LLMs’
instruction-following abilities (Chen et al., 2024a;
Jiang et al., 2024; Wen et al., 2024), they mainly fo-
cused on style constraints, such as the format (Zhou
et al., 2023), length, or topic of responses. Yet, the
significance of instruction-following extends well
beyond style compliance. In many problem-solving
scenarios, instructions function as rules: they im-
pose logical constraints on the reasoning process
and specify how answers should be derived from
given inputs. However, limited attention has been
paid to LLMs’ capacity to follow complex rules.
Existing research (Mu et al., 2023; Sun et al.,
2024) largely addresses only single-step, first-order
logic reasoning or artificially synthesized logical
tasks. In contrast, real-world rules frequently ap-
pear in diverse and nuanced natural language forms.
They may involve intricate logical structures, in-
cluding the need for parallel reasoning across multi-
ple rules or navigating interdependent rule sets. For
instance, to calculate the fees for checked luggage
when taking flights, one needs to consider the base
price for checking each item, the overweight and
oversize charges, and how these charges should be
aggregated together. The extent to which LLMs can
accurately follow these complex, real-world rules—
an ability we term rule-guided reasoning—remains
unknown. To better understand the complexity
and practical implications of rule-guiding reason-
ing, we introduce a new evaluation benchmark,
RULE ARENA , grounded in realistic scenarios.
1https://www.theguardian.com/world/2024/feb/
16/air-canada-chatbot-lawsuit
550

TaskInstruction
ReferenceRules
Assistant
UserRule-GuidedReasoning
To calculate the total cost for Alice, we consider the flight ticket, checked bag fees, and overweight or oversize fees according to thegivenpolicies.⋯⋯⋯⋯Fees for Each Checked Bag:                                                 1. Item 2 (Luggage box):           -Total dimensions: 32+18+13= 63inches-Over the standard size limit of 62 inches.-Weight: 58lbs(Over53lbsto70 lbs).-Checking Fee:-For the first checked bag, the fee is $100.-Overweight Fee:-For bagover 53 lbsto 70 lbs, the fee is $100.-Total Fee for Item 2:⋯⋯⋯⋯⋯⋯
Youaregiventhe information of a passenger, his / her items and the policies of American Airlines.You should compute the total cost (including the flight ticket feeandchecked bag fees) according to the policies for the passenger.
TestInstance
Alice is a MainCabinpassenger flying fromParistoLos Angeles with the following items:1. A backpack: 20 x 12 x 6 inches, 10 lbs;2. A luggage box: 32 x 18 x 13 inches, 58 lbs
⋯⋯⋯⋯Carry-on bagsYou're allowed 1 carry-on bag and 1 personal item in all cabins.Firstcheckedbag
^Main Plus includes 1 extra free checked bag in addition to the Main Cabin allowance (max of 2).⋯⋯⋯⋯
AirlineNBA TaxChallengingReal-WorldScenarios
Figure 1: Overview of RULE ARENA . RULE ARENA contains 95 commonly used and moderately complex rules
and 816 test problems from three representative real-world scenarios - airline luggage fees, NBA transactions, and
taxation policies. LLMs are given a the task instruction, the reference rules in this scenario, and a user instance, and
required to conduct reasoning and computation for the user input under the guidance of reference rules.
As illustrated in Figure 1, RULE ARENA is de-
veloped from three representative, real-world sce-
narios: (1) airline luggage fee policies, (2) NBA
transactions, and (3) taxation policies. From these
domains, we collect authentic rules currently im-
plemented by companies or government agencies.
For each domain, we construct a set of challenging
test problems, pairing each question with a ground-
truth solution, and then evaluate a range of state-
of-the-art LLMs on their ability to conform to the
rules. LLMs are provided with the domain-specific
task instructions and reference rules, and required
to resolve each test problem through reasoning ac-
cording to the question and reference rules.
Our contributions in this work can be summa-
rized in three main points:
• A diverse collection of real-world rules:We as-
semble a comprehensive set of 95 policies/rules
drawn from these three real-world scenarios.
• A challenging benchmark and novel evalu-
ation metrics: Using the collected rules, we
introduce RULE ARENA , a new benchmark con-
taining 816 datapoints designed to test LLMs’
rule-guided reasoning ability. We further pro-
pose a suite of evaluation metrics for both rule
selection and rule application, providing fine-
grained insights into LLMs’ performance.
• A comprehensive analysis of prevalent chal-
lenges: By examining common failure cases,
identifying difficult rule types, and conducting
extensive controlled experiments, we uncover
several systematic issues that limit current LLMs’
rule-guided reasoning capabilities and promising
directions to improve.
2 Related Work
Complex Instruction-following Benchmarks A
wide range of benchmarks has been designed
to evaluate LLMs’ instruction-following abilities
from various perspectives, including semantics
(Zheng et al., 2023; Li et al., 2023; Liu et al.,
2023; Wu et al., 2024a,b), format (Xia et al., 2024;
Tang et al., 2023), and response length (Chen et al.,
2024b; Sun et al., 2023). To further probe com-
plexity, some works have introduced benchmarks
that construct complex instructions through com-
positional methods. For example, WizardLM (Xu
et al., 2023) generates intricate tasks by combin-
ing simpler instructions, while CELLO (He et al.,
2024) uses task descriptions and input texts to
create complex prompts grounded in real-world
scenarios. ComplexBench (Wen et al., 2024)
adopts multiple compositional structures to inte-
grate atomic requirements into more challenging
551

instructions. In contrast, our work focuses on in-
structions derived directly from real-life scenarios,
where naturally occurring complexities arise from
multifaceted constraints incurred by inputs on the
set of instructions.
Logical Reasoning Benchmarks Extensive re-
search has explored benchmarks for mathematical
(Koncel-Kedziorski et al., 2016; Ling et al., 2017;
Amini et al., 2019; Cobbe et al., 2021; Hendrycks
et al., 2021) and logical (Mao et al., 2019; Gupta
et al., 2020; Tafjord et al., 2021; Zhong et al., 2022;
Han et al., 2022; Zhang and Ding, 2024) reason-
ing, evaluating LLMs’ abilities to solve math prob-
lems of varying difficulty, tackle coding challenges,
and engage in deductive logic. Although these
benchmarks test models’ reasoning skills, their log-
ical constraints are often represented in simplified,
formal systems, such as propositional (Hua et al.,
2024) or first-order logic (Zhu et al., 2023; Mu
et al., 2023; Sun et al., 2024). In contrast, our
benchmark deals with rules that arise in natural
language, capturing a richer, more realistic set of
constraints. Such natural language rules extend be-
yond neatly formalized logical representations, of-
ten express higher-order logic and more intricate re-
lationships than typical propositional or first-order
logic formalizations.
3 RuleArena
In this section, we present the RULE ARENA bench-
mark and its construction process. We begin by
describing the domains we have chosen and the
corresponding regulations from which our rules are
collected. We then describe how problems with
varying difficulty levels are generated and how the
ground-truth solutions are computed. Finally, we
present the evaluation metrics we used to evaluate
whether correct rules are correctly applied.
3.1 Domains and Rule Collection
We select three real-life domains that all both fa-
miliar in everyday life and demonstrate a high level
of complexity:
Airline. It requires LLM to calculate the total
cost for one or more passengers, including their
flight ticket and checked baggage fees. The regu-
lations are extracted from policy of American Air-
lines2. The complexity stems from the fact that bag-
2https://www.aa.com/i18n/customer-service/
support/optional-service-fees.jsp
Airline NBA Tax
# Rules 10 54 31
Average # Tokens 376 398 359
Table 1: Statistics of rules in each domain.
gage costs vary according to factors such as cabin
class, flight origin and destination, the number of
checked bags, and the size of each bag. Conse-
quently, LLMs must carefully identify the correct
baggage-related rules and apply them accurately to
determine the final cost.
NBA transaction. It requires LLMs to determine
whether one or more specified transactions are al-
lowed. The regulations are extracted from the 2023
NBA Collective Bargaining Agreements 3 (CBA)
and excerpt from the NBA Constitution and By-
Laws4. Complexity arises from the numerous fac-
tors influencing transaction eligibility, including
the player’s contract value, salary-matching con-
straints, and the specific transaction date. LLMs
must accurately identify and apply the relevant
rules from the agreement to determine whether a
given transaction can proceed.
Tax. It requires LLMs to calculate the income tax
for one person or family given their financial infor-
mation. The regulations are collected from Internal
Revenue Service5. Although taxes are a common
and universally encountered aspect of modern life,
they are also known for their complexity. This
complexity stems from a wide range of factors,
including salary income, investment gains, gifts,
home ownership and related expenses, as well as
the jurisdiction in which income is earned. To ar-
rive at the correct tax amount, LLMs must navigate
and apply the appropriate rules drawn from these
multifaceted conditions.
The statistics for the collected rules are summa-
rized in Table 1. Although the total number of rules
is relatively small, each rule averages just under
400 tokens in length, tokenized by Llama-3.1 to-
kenizer. This presents a substantial challenge for
both rule comprehension and the handling of long
contexts. For more details on how we collect our
rules, please refer to Appendix B.1.
3https://ak-static.cms.nba.com/
wp-content/uploads/sites/4/2023/06/
2023-NBA-Collective-Bargaining-Agreement.pdf
4https://ak-static-int.nba.com/
wp-content/uploads/sites/3/2015/12/
NBA-Constitution-and-By-Laws.pdf
5https://www.irs.gov/forms-instructions
552

3.2 Problem Annotation
After gathering the relevant rules for each domain,
we construct challenging test problems designed to
evaluate whether LLMs can produce correct out-
puts from the provided rules.
Airline. The problems are generated by ran-
domly selecting passenger information (e.g., cabin
class, itinerary, ticket price) and their checked bag-
gage details (e.g., dimensions, weight). We convert
each regulation into a corresponding rule-based
script, enabling the direct calculation of ground-
truth answers by executing these scripts. LLM
performance is then assessed by comparing the
model’s computed solutions to the script-derived
ground truths, step by step.
NBA Transaction. The problems consist of pro-
posed trades that may or may not comply with NBA
regulations. Because these problems require a wide
variety of operations and rule sets, fully automated
generation and evaluation are difficult. Therefore,
we employ annotators familiar with NBA transac-
tion rules to curate complex test cases and identify
all the relevant rules needed to resolve each case
(further details in Appendix B.2). For each prob-
lem, we ask the LLM whether the transaction is
legit or not based on the regulations. If LLM thinks
the transaction is legit, it should generate “Yes”;
otherwise, it needs to identify the specific team and
transaction that violates the rules.
Tax. The problems are randomly generated from
hypothetical taxpayer profiles including informa-
tion such as income levels, filing status, etc. IRS
tax regulations are translated into rule-based scripts
to compute ground-truth tax obligations. As with
the airline scenario, we measure LLM accuracy by
comparing the model’s step-by-step calculations
with those derived directly from the scripts.
3.3 Difficulty control
To assess LLMs’ capabilities under varying levels
of complexity, we create problems with different
degrees of difficulty. We define three levels of
difficulties in each domain.
Airline. The difficulty is controlled by adjusting
the number of bags a passenger carries.
NBA Transaction. Complexity is determined by
increasing the number of teams, players, and trans-
actions involved in a scenario.
Tax. The level of difficulty is raised by progres-
sively introducing additional tax forms and thus
relevant regulations.
The statistics of problems at different difficulty
levels in each domain are listed in Table 2.
Airline NBA Tax
Level 1 100 81 100
Level 2 100 89 100
Level 3 100 46 100
In Total 300 216 300
Table 2: Number of test problems at different difficulty
levels in each domain.
3.4 Evaluation Metrics
To achieve a comprehensive evaluation of the
rule-following abilities of Large Language Models
(LLMs), we introduce a set of evaluation metrics.
Unlike existing benchmarks (Hua et al., 2024; Fan
et al., 2023; Zhu et al., 2023), which primarily
rely on simple metrics such as answer accuracy or
BLEU scores, our approach aims to conduct a more
detailed analysis of the step-by-step rule-guided
reasoning process. This analysis includes exam-
ining each rule application to determine whether
the rule should be applied, whether any rules are
missed, and whether the rule application computa-
tion process is accurate.
For each domain, assuming a setT of N prob-
lems and a set R of M. For each problem ti =
(qi, ai, Ri), we have a query qi, an answer ai, and a
set of relevant rules Ri, together with a rule-usage
matrix U∈RN×M, where each item Ui,r∈{0,1}
indicates whether a rule r is used by an LLM in
problem ti. Matrix U can be approximately ob-
tained by parsing LLMs’ responses using an LLM,
which we will introduce in Section 4.1.
Now we introduce two groups of metrics:
The first group focuses on problem-level eval-
uations: for each problem, we examine whether
all necessary rules were applied, whether any ex-
traneous rules were applied, and whether the final
answer aligns with the ground-truth solution:
Problem-wise Recall: denoted as R(t), measures
whether LLMs apply all relevant rules for a prob-
lem t. For each problem ti, P(ti) is calculated as
the proportion of relevant rules that are applied by
LLMs:
R(ti) =
∑
r∈Ri I(Ui,r= 1)∑
r I(r∈Ri) (1)
553

Problem-wise Rule Application Correctness: de-
noted as AC(t), measures whether LLMs apply
rules correctly for a problem t. For each problem
ti, AC is calculated as the proportion of correctly
applied rules that are relevant:
AC(ti) =
∑
Ui,r=1 I(r is correctly applied)
∑
r I(Ui,r= 1) (2)
Problem-wise Precision: denoted as P(t), mea-
sures whether LLMs apply only relevant rules for a
problem t. For each problem ti, P(ti) is calculated
as the proportion of applied rules that are relevant:
P(ti) =
∑
Ui,r=1 I(r∈Ri)
∑
r I(Ui,r= 1) (3)
Problem-wise Accuracy: denoted as Acc(t), mea-
sures whether LLMs accurately answer the prob-
lem t comparing with ground-truth result. Assume
the LLM provides answer ˜ai for a problem ti, the
accuracy should be calculated as:
Acc(ti) = I(˜ai = ai) (4)
The second group of metrics focuses on rules
rather than problems. For each rule in the do-
main, we assess whether it is applied to all prob-
lems that require it, and whether the problems it is
applied to are exactly those that necessitate it:
Rule-wise Recall , denoted as R(r), measures
whether LLMs decide to apply r when r is rele-
vant to a problem:
R(r) =
∑
i I(r∈Ri)I(Ui,r= 1)∑
i I(r∈Ri) (5)
Rule-wise Rule Application Correctness , de-
noted as AC(r), measures whether LLMs correctly
apply r:
AC(r) =
∑
i I(Ui,r= 1)I(r is correctly applied)∑
i I(Ui,r= 1)
(6)
Rule-wise Precision, denoted as P (r), measures
whether r is relevant to the problem when LLMs
decide to apply r:
P(r) =
∑
i I(Ui,r= 1)I(r∈Ri)∑
i I(Ui,r= 1) (7)
Problem Recall and Problem Accuracy are ap-
plied to all three domains to measure the ability of
LLMs to match and aggregate rules and to compre-
hensively follow the rules. Problem Application
Correctness is used on Airline and Tax tasks to
evaluate whether LLMs can operate correctly un-
der the guidance of rules, as these two tasks have
clear procedures without ambiguous rules, while
Problem Precision is used on NBA tasks to exam-
ine whether LLMs can differentiate similar rules
applicable to different situations.
4 Experiments
This section presents the experiments on bench-
mark. We first introduce the LLMs and prompting
strategies we use to evaluate, and then present the
evaluation result.
4.1 Experiment Settings
LLMs. Our rules, which are prompted directly
into LLMs, can be of a length up to 20,000 to-
kens. Therefore, we only consider LLMs that can
handle such long contexts, including Llama-3.1
70B, Llama-3.1 405B (Dubey et al., 2024), Qwen-
2.5 72B (Qwen Team, 2024), Claude-3.5 Sonnet
(Anthropic, 2024), GPT-4o (OpenAI, 2024a), and
o1-preview (OpenAI, 2024b).
Prompting Strategies. Since rule-guided reason-
ing can be an intricate multi-step reasoning process
in our three real-world scenarios, we use Chain-
of-Thought (CoT) (Wei et al., 2022; Kojima et al.,
2022) reasoning by default. To further study if
LLMs can learn to follow hard rules through in-
context examples, we also compare 0-shot with
1-shot CoT given an example including a task of
the lowest difficulty and its solution. Due to con-
text limit, we do not further increase the number of
in-context examples.
Output Parsing. To obtain the rule-usage matrix
U we mentioned in Section 3.4, we utilize the struc-
tured output mode of GPT-4o (OpenAI, 2024a) to
parse the raw textual responses from LLMs. Specif-
ically, for airline and tax problems we structuralize
the ground-truth calculation process and ask GPT-
4o to fill in problem-specific information according
to an LLM’s response, while for NBA problems
we enumerate a list of all rules and ask GPT-4o
to directly judge whether a specific rule is applied
in an LLM’s response. For details we refer our
readers to Appendix C.
4.2 Main Results
This section provides a comprehensive analysis of
benchmark results. The analysis is divided into
two parts: problem-wise analysis and rule-wise
554

Models Settings Level 1 Level 2 Level 3
P(t) AC(t) R( t) Acc(t) P( t) AC(t) R( t) Acc(t) P( t) AC(t) R( t) Acc(t)
Airline
Llama-3.1 70B0-shot 1.000 0.764 0.558 0.01 1.000 0.732 0.535 0.01 1.000 0.752 0.578 0.00
1-shot 1.000 0.809 0.787 0.17 1.000 0.827 0.801 0.07 1.000 0.769 0.815 0.01
Qwen-2.5 72B0-shot 1.000 0.636 0.586 0.01 1.000 0.627 0.554 0.01 1.000 0.588 0.544 0.00
1-shot 1.000 0.836 0.908 0.19 1.000 0.818 0.901 0.10 1.000 0.801 0.904 0.01
Llama-3.1 405B0-shot 1.000 0.854 0.604 0.03 1.000 0.844 0.587 0.06 1.000 0.845 0.570 0.01
1-shot 1.000 0.919 0.921 0.32 1.000 0.897 0.905 0.16 1.000 0.870 0.946 0.04
Claude-3.5 Sonnet0-shot 1.000 0.930 0.702 0.04 1.000 0.876 0.669 0.00 1.000 0.888 0.646 0.01
1-shot 1.000 0.960 0.871 0.29 1.000 0.966 0.822 0.30 1.000 0.972 0.718 0.11
GPT-4o 0-shot 1.000 0.862 0.616 0.02 1.000 0.868 0.578 0.00 1.000 0.813 0.548 0.00
1-shot 1.000 0.922 0.885 0.32 1.000 0.875 0.853 0.16 1.000 0.835 0.798 0.05
o1-preview 0-shot 1.000 0.968 0.888 0.54 1.000 0.950 0.881 0.37 1.000 0.958 0.855 0.21
1-shot 1.000 0.971 0.911 0.63 1.000 0.963 0.901 0.55 1.000 0.961 0.929 0.46
NBA Transaction
Llama-3.1 70B0-shot 0.579 – 0.428 0.40 0.498 – 0.246 0.36 0.540 – 0.250 0.22
1-shot 0.560 – 0.565 0.49 0.466 – 0.386 0.25 0.578 – 0.438 0.26
Qwen-2.5 72B0-shot 0.556 – 0.409 0.44 0.537 – 0.339 0.43 0.592 – 0.305 0.30
1-shot 0.595 – 0.526 0.53 0.495 – 0.378 0.35 0.574 – 0.327 0.17
Llama-3.1 405B0-shot 0.581 – 0.419 0.49 0.577 – 0.323 0.30 0.561 – 0.297 0.28
1-shot 0.608 – 0.550 0.56 0.559 – 0.439 0.29 0.575 – 0.461 0.10
Claude-3.5 Sonnet0-shot 0.660 – 0.457 0.38 0.630 – 0.373 0.40 0.588 – 0.292 0.28
1-shot 0.676 – 0.528 0.58 0.676 – 0.410 0.47 0.650 – 0.371 0.26
GPT-4o 0-shot 0.650 – 0.446 0.40 0.570 – 0.327 0.26 0.603 – 0.291 0.24
1-shot 0.616 – 0.506 0.40 0.597 – 0.392 0.28 0.569 – 0.318 0.20
o1-preview 0-shot 0.742 – 0.502 0.44 0.707 – 0.430 0.47 0.747 – 0.415 0.24
1-shot 0.731 – 0.565 0.58 0.715 – 0.512 0.40 0.724 – 0.413 0.20
Tax
Llama-3.1 70B0-shot 1.000 0.834 0.989 0.01 1.000 0.767 0.918 0.00 1.000 0.745 0.852 0.00
1-shot 1.000 0.923 0.998 0.11 1.000 0.895 0.941 0.00 1.000 0.873 0.910 0.00
Qwen-2.5 72B0-shot 1.000 0.888 0.998 0.10 1.000 0.835 0.944 0.01 1.000 0.785 0.903 0.00
1-shot 1.000 0.931 1.000 0.17 1.000 0.919 0.934 0.00 1.000 0.921 0.868 0.00
Llama-3.1 405B0-shot 1.000 0.923 0.999 0.16 1.000 0.876 0.964 0.02 1.000 0.797 0.926 0.00
1-shot 1.000 0.941 1.000 0.24 1.000 0.914 0.958 0.03 1.000 0.873 0.880 0.00
Claude-3.5 Sonnet0-shot 1.000 0.964 1.000 0.32 1.000 0.934 0.940 0.02 1.000 0.887 0.866 0.00
1-shot 1.000 0.979 1.000 0.64 1.000 0.954 0.969 0.16 1.000 0.895 0.888 0.00
GPT-4o 0-shot 1.000 0.965 1.000 0.42 1.000 0.951 0.957 0.07 1.000 0.945 0.908 0.00
1-shot 1.000 0.975 1.000 0.57 1.000 0.975 0.944 0.07 1.000 0.982 0.893 0.00
o1-preview 0-shot 1.000 0.992 1.000 0.72 1.000 0.945 0.981 0.28 1.000 0.914 0.976 0.19
1-shot 1.000 0.994 1.000 0.68 1.000 0.960 0.994 0.33 1.000 0.894 0.939 0.24
Table 3: Main problem-wise evaluation results on airline, NBA, and tax domains. P(t) denotes problem-wise
precision, AC(t) denotes problem-wise rule application correctness, and R(t) denotes problem-wise recall. The
best and second best results in each column are rendered with orange and blue backgrounds respectively.
analysis. The problem-wise analysis evaluates the
performance of LLMs across different problems
and difficulty levels; the rule-wise analysis delves
into how effectively LLMs identify and apply spe-
cific rules, highlighting common failure modes and
the impact of rule complexity and similarity.
4.2.1 Problem-wise Analysis
Table 3 presents the evaluation results 6. Notice
that the values of precision ( P)(t), rule applica-
tion (AC(t)), and recall ( R(t)) are much higher
than accuracy (Acc(t)). This is because solving a
problem requires using multiple rules, hence one
correct rule recall or application is insufficient for a
6In NBA domain, the problem-wise correctness (AC(t))
of rule application could not be computed due to the absence
of step-by-step computation annotations. Generating such
detailed annotations would require extensive human effort.
correct answer. For example, if a problem requires
10 rules and only one rule is missed,R(t) is high as
0.9 while very probably leading to mistaken final
answer (Acc(t) = 0 ).
Low Accuracy. Overall performance in problem
result accuracy (Acc(t)), as summarized in Table 3,
remains unsatisfactory across all three scenarios.
Under the 0-shot setting, non-reasoning LLMs such
as Llama 405B, Claude-3.5, and GPT-4o fail to
produce correct answers for the simplest test prob-
lems, and even advanced reasoning model like o1-
preview can solve only about 50%∼60% of Level 1
problems. For more challenging problems, par-
ticularly in the airline and tax domains, Acc(t)
of non-reasoning models rarely exceeds 10% and
o1-preview fails most of them as well. In 1-shot
setting, we notice marked improvements on the
easiest problems, yet the gains diminish as prob-
555

lem difficulty increases. These persistently low
Acc(t) highlight the inherent complexity of the
RULE ARENA benchmark and emphasize the need
for more robust LLM reasoning and rule-following
capabilities.
High Precision. In both the airline and tax sce-
narios, LLMs achieve 100% precision ( P(t)) in
rule selection precision, consistently applying only
those rules that are required. We notice that high
P(t) stems from the relative clarity of the rules in
these domains; the rules are neither highly similar
nor ambiguous, making it straightforward to de-
termine which ones apply. In contrast, the NBA
scenario presents a more challenging environment,
leading to noticeably lower P(t) in rule selection.
Low Recall. Despite exhibiting high P(t) in cer-
tain domains, LLMs often struggle with rule recall
(R(t)). Low R(t) in the airline, NBA, and more
complex tax problems indicate that models do not
fully grasp the reasoning workflows required. Con-
sequently, they frequently fail to recall all neces-
sary rules, reflecting an incomplete or superficial
understanding of the underlying logic.
High Rule Application Correctness. While
LLMs demonstrate relatively high application cor-
rectness (AC(t)) on rule application computation
on average, AC(t) never reaches a perfect 100%.
Occasional errors in mathematical calculations or
logical operations emerge even under explicit rule
guidance. Although these mistakes are not perva-
sive, a single computational error can significantly
compromise the final output’s accuracy (Acc(t))
in many cases. This observation underscores the
importance of improving the reliability of math
computation abilities in LLMs.
4.2.2 Rule-wise Analysis
Here, we provide a detailed examination of the rule-
level evaluation. Figure 2 presents recall ( R(r))
and application correctness (AC(r)) in airline do-
main and metrics for NBA transaction and tax do-
mains are presented in Figure 5 and Figure 6 in Ap-
pendix. Table 4 summarizes the metric results by
reporting the mean and variance of three key met-
rics across all rules: recall (R(r)), application cor-
rectness (AC(r)), and precision ( P(r)). The low
variance observed in metrics such as P(r) within
the airline and tax domains suggests that certain
performance aspects are largely independent of the
specific rules being applied. In contrast, the high
variance seen in metrics like R(r) implies that re-
call performance is significantly influenced by the
0.0 0.2 0.4 0.6 0.8 1.0
Recall
Rule 1
Rule 2
Rule 3
Rule 4
Rule 5
Rule 6
Rule 7
Rule 8
Rule 9
Rule 10Rule
Rule 1 -- overall fee aggregation
Rule 2 -- 1st check bag fee
Rule 3 -- 2nd check bag fee
Rule 4 -- overweight fee
Rule 5 -- 4+ check bag fee
Rule 6 -- main plus extra free bag
Rule 7 -- 3rd check bag fee
Rule 8 -- oversize fee
Rule 9 -- complementary overweight
Rule 10 -- maximum violation fee
(a) Recall
0.0 0.2 0.4 0.6 0.8 1.0
Precision
Rule 1
Rule 2
Rule 3
Rule 4
Rule 5
Rule 6
Rule 7
Rule 8
Rule 9
Rule 10Rule
Rule 1 -- complementary overweight
Rule 2 -- maximum violation fee
Rule 3 -- 4+ check bag fee
Rule 4 -- 1st check bag fee
Rule 5 -- 2nd check bag fee
Rule 6 -- oversize fee
Rule 7 -- 3rd check bag fee
Rule 8 -- overweight fee
Rule 9 -- overall fee aggregation
Rule 10 -- main plus extra free bag
(b) Correctness
Figure 2: Rule-wise metrics of rules in airline domain.
particular rules in question. Detailed analysis is
presented below in Table 4.
Airline NBA Tax
Mean(P(r)) 1.000 0.504 1.000
Var(P(r)) 0.000 0.110 0.000
Mean(Ac(r)) 0.798 – 0.828
Var(Ac(r)) 0.026 – 0.047
Mean(R(r)) 0.721 0.308 0.900
Var(R(r)) 0.109 0.082 0.050
Table 4: Statistics of our three rule-wise metrics.
Rules with Low Recall. Certain rules are sys-
tematically overlooked across multiple data points,
indicating that their neglect is not random but con-
centrated on specific rules. To understand which
rules are most frequently disregarded, we identify
the top-5 rules with the lowestrecall (R(r)), as pre-
sented in Table 5. We find that most of these rules
are “non-essential,” meaning they apply only under
specific conditions. In contrast, “essential” rules
must be applied in every scenario. For example, in
the airline domain, essential rules define the base-
line costs for each piece of luggage and the flight
itself, making them relevant to all situations. Con-
versely, rules pertaining to overweight or oversized
baggage only apply when such conditions arise,
rendering them non-essential. Our observations in-
dicate that these scenario-dependent, non-essential
rules are more frequently neglected.
Rule with Low Application Correctness. We
also identified the top-5 rules with the lowest cor-
rectness (AC(r)), listed in Table 6. The majority
of these rules are “compositional” in nature, re-
556

Airline NBA Tax
Rule essential Rule essential Rule essential
maximum violation fee salary space consumption of bird right education credits
complementary overweight salary space consumption of early bird right american opportunity credit
oversize fee sign and trade maximum salary ✓ net profit
3rd base check fee ✓ Arenas provision ctc or other dependent credit
main plus extra free bag over 38 rule taxes with qualified dividends ✓
Table 5: Top-5 rules of the lowest recall in ascent order of recall.
Airline Tax
Rule Composition Rule Composition
main plus extra free bag ✓ taxes with qualified dividends ✓
overall fee aggregation ✓ standard taxes
overweight fee matching itemized deductions ✓
3rd base check fee standard deductions ✓
oversize fee matching ✓ total income ✓
Table 6: Top-5 rules of the lowest correctness in ascent order of correctness.
quiring the aggregation of at least two previously
computed intermediate results. By contrast, “non-
compositional” rules demand at most a single math-
ematical operation involving a single intermediate
result. Our analysis shows that compositional rules
yield significantly lower AC(r) scores, indicating
that LLMs struggle more with problems involving
multiple reasoning steps than with straightforward,
one-step computations.
Rules with Low Precision. In both the airline
and tax scenarios, all rules exhibit high precision
(P(r)), indicating that LLMs rarely apply irrelevant
rules during the reasoning process. However, the
NBA domain presents a different challenge, where
multiple rules appear similar. As shown in Table 7,
rules with low precision in the NBA domain usually
have alternatives applicable under different condi-
tions in the same situation. This pattern suggests
that when rules are easily confused with one an-
other, LLMs struggle to consistently identify and
apply the correct one.
Rule Substitutable
higher max criterion
non bird right ✓
taxpayer mid level exception hard cap ✓
standard traded player exception ✓
salary increase ratio except bird right ✓
Table 7: Top-5 rules of the lowest precision in ascent
order of precision.
4.3 In-Depth Analyses
4.3.1 What Impacts Rule Following?
We study the factors influencing LLM performance,
as measured by Acc(t), and provide the complete
experiment results in Appendix D.2. The main
findings are:
Correlation between Acc(t) and other metrics.
We compare the correlation between problem-wise
metrics (i.e., P(t), AC(t), R(t)) and accuracy
Acc(t). The correlation is the most obvious and al-
most linear between R(t) and Acc(t), while highly
non-linear or unclear between other two metrics
and Acc(t).
The effect of in-context examples. We observe
that LLMs generally provide better performances
given 1-shot example on airline, tax, and (easy)
NBA problems. However, when tackling more chal-
lenging NBA problems (Levels 2 and 3), providing
an example increases P(t) and R(t) but leads to a
counterintuitive decrease in overall Acc(t).
Rule representation has a mild effect. In the air-
line and tax domains, some rules are represented
as Markdown tables. To test whether representa-
tion format affects performance, we convert these
tabular rules into textual “if-then” statements and
compare with original results. The comparison
shows that converting tabular rules into text im-
proves R(r), but has little impact on other metrics,
including Acc(t).
Distractive rules degrades LLM performance.
An essential aspect of rule-following involves iden-
tifying which rules are relevant to the current prob-
557

RuleRecallRelevantRule:More than one fee may apply per bag exceedingstandard dimensions for both weight and size. The higher between oversize and overweight applies, in addition to the checked bag fee.
ErrorCase:-Bag 2: 36 x 14 x 14 inches, 98 lbs-Dimensions: 64 inches (36 + 14 + 14)-Weight: 98 lbs-Overweight fee (over 70 lbs): $200-Oversize fee (over 62 inches): $30-Total fee for Bag 2: $200 + $30 = $230
Revision:-Bag 2: 36 x 14 x 14 inches, 98 lbs-Dimensions: 64 inches (36 + 14 + 14)-Weight: 98 lbs-Overweight fee (over 70 lbs): $200-Oversize fee (over 62 inches): $30-Total fee for Bag 2: Max($200,$30)= $200
RuleMisuseRelevantRule:In the eventthe Team’sTeam Salaryis below the Salary Capsuch that the Team is not entitled to use theNon-TaxpayerMid-LevelSalaryException,theTeammayuse the Mid-Level Salary Exception for Room Teams.ErrorCase:Salary Cap Exception:-Team C’steam salary is$140,000,000, belowthe Salary Cap.-Theycan use the Non-Taxpayer Mid-Level Salary Exceptionto sign Player E,as the first-year salary is within the limit of 9.12% of the Salary Cap ($12,812,569).Revision:Salary Cap Exception:-Team C’steam salary is$140,000,000, belowthe Salary Cap.-Theycan use the Mid-Level Salary ExceptionforRoomTeamsto sign Player E,as the first-year salary is within the limit of 5.678% of the Salary Cap ($7,982,587).
ComputationRelevantRule:If your taxable income is less than $100,000, you must use the Tax Table to figure your tax. If your taxable income is $100,000 or more, use the Tax Computation Worksheet right after the Tax Table.ErrorCase:Since the payer‘s taxable income is more than $100,000, and the filing status is is Married filing separately, we use the Tax Computation Worksheet to figure the tax.Line 16 -Tax: $146,144 * 0.24 = $35,094.56
Revision:Since the payer‘s taxable income is more than $100,000, and the filing status is is Married filing separately, we use the Tax Computation Worksheet to figure the tax.Line 16 -Tax: $146,144 * 0.24 = $35,074.56
Figure 3: Failure Case Studies. Existing LLMs commonly fail due to inadequate rule recall, inappropriate usage of
similar rules, and computation errors.
lem. We assess the extent to which irrelevant rules
detrimentally affect performance, and notice that
the presence of distractive (irrelevant) rules signifi-
cantly degrades LLM performance.
Tool augmentation boosts overall performance.
Using external tools is a simple way to reduce math
and logic errors from LLMs. To study to what ex-
tent external tools can help LLM rule-guided rea-
soning, we ask our LLMs to write Python code and
use the execution result as solution, where Python
interpreter serves as an oracle math and logic cal-
culator. We observe that LLMs can achieve a sig-
nificant performance boost with tool augmentation
but are still far from perfect.
4.3.2 Case Studies
To gain an intuitive understanding of how and why
LLMs fail in complex rule-following problems,
we present representative failure cases in Figure
3. These examples highlight three frequently ob-
served failure modes:
LLMs fail to recall certain rules. As discussed
in Section 4.2, LLMs often neglect non-essential
rules. In airline problems, for instance, a crucial
requirement is to apply either the oversize fee or
the overweight fee (whichever is higher) and not
to sum them. However, LLMs frequently overlook
this instruction and incorrectly combine both fees,
resulting in an inflated, incorrect total cost.
LLMs get confused by similar rules. When mul-
tiple rules appear similar but are applicable un-
der different conditions, LLMs can misapply them.
For example, in the NBA domain, teams under the
Salary Cap should use the Mid-Level Exception
for Room Teams, whereas teams above the Salary
Cap should apply the Non-Taxpayer Mid-Level Ex-
ception. As illustrated in the second failure case of
Figure 3, LLMs sometimes conflate these excep-
tions. Similar confusion also arises with various
Traded Player Exceptions and differing types of
Bird Rights.7
LLMs compute incorrect results. Mathematical
and logical operations present ongoing challenges.
For example, in the tax scenario, LLMs must accu-
rately compute a series of values related to income,
tax brackets, and credits. Even a minor arithmetic
mistake compromises the final result, as shown in
the third failure case. Such computational errors
underscore the need for more precise and reliable
reasoning capabilities in LLMs.
5 Conclusions
In this paper, we introduce RULE ARENA , a real-
world benchmark designed to evaluate the abili-
ties of LLMs on various rule-guided reasoning
tasks. We observe that existing LLMs face sig-
nificant challenges when they try to tackle prob-
lems on RULE ARENA - even the strongest Claude-
3.5 and GPT-4o models can hardly succeed on
our hardest tasks. Our further analysis indicates
that LLMs struggle to integrate multiple rules or
facts cohesively and are prone to irrelevant distrac-
tions. RULE ARENA poses fundamental challenges
in complex rule following, and provides a valuable
tool for understanding and enhancing the reasoning
abilities of LLMs. We envision RULE ARENA as a
foundation for future research to improve LLM per-
formances in solving increasingly complex tasks.
7Explanations of these specific NBA terms can be found
in the Appendix A.
558

Limitations
While this study provides a comprehensive evalua-
tion and analysis of LLMs’ rule-guided reasoning
capabilities, there remain some limitations and nu-
merous promising avenues for future research:
Automating Evaluation In this work, we only
rely on GPT-4o to parse textual responses into struc-
tured JSON, facilitating downstream analyses of
rule application. A logical next step would be to
investigate the use of LLMs for fully automated rea-
soning evaluations, including the identification of
intermediate errors. This direction aligns with the
concept of “LLM-as-a-judge” (Zheng et al., 2023),
which, despite potential bias or inaccuracies (Xu
et al., 2024), offers a scalable alternative to labor-
intensive human evaluation and could improve the
reliability and granularity of assessment metrics.
Training with Rule-Guided Reasoning Data
Supervised fine-tuning has proven effective in en-
hancing LLMs’ performance on tasks requiring
substantial domain knowledge (Jeong, 2024; Fu
et al., 2023; Wu et al., 2024a). While we did not
pursue fine-tuning in this study—given the high
cost of obtaining extensive rule-guided reasoning
data and the limited generalizability to unseen do-
mains—it remains a worthwhile direction. Investi-
gating whether training with datasets from related
domains, such as mathematical or logical reason-
ing tasks or code generation problems, can bolster
LLMs’ rule-following performance is an open ques-
tion worth exploring.
Sophisticated Rule Recall and Aggregation
Our experiments reveal that LLMs frequently strug-
gle with recalling and aggregating the correct rules,
and that problem-wise recall strongly correlates
with overall accuracy. Addressing these challenges
may involve refining rule retrieval mechanisms or
integrating structured reasoning frameworks. For
instance, approaches that retrieve relevant informa-
tion dynamically (Trivedi et al., 2023; Zhou et al.,
2024) and convert rules into structured data (Pan
et al., 2023; Wang et al., 2024) have shown promise
in reasoning tasks. Building on these insights, a
hybrid system that combines LLM-based reasoning
with symbolic reasoners may enhance both the con-
sistency and robustness of rule-guided reasoning in
real-world scenarios.
References
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. 2019. Mathqa: Towards interpretable math
word problem solving with operation-based for-
malisms. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 2357–2367.
Anthropic. 2024. The claude 3 model family: Opus,
sonnet, haiku. Claude-3 Model Card.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W. Cohen. 2023. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. Transactions on
Machine Learning Research.
Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, and
Zhendong Mao. 2024a. Benchmarking large lan-
guage models on controllable generation under diver-
sified instructions. In Proceedings of the 38th AAAI
Conference on Artificial Intelligence (AAAI), pages
17808–17816.
Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, and
Zhendong Mao. 2024b. Benchmarking large lan-
guage models on controllable generation under diver-
sified instructions. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 38, pages
17808–17816.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan
Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu,
Tianyu Liu, et al. 2022. A survey on in-context learn-
ing. arXiv preprint arXiv:2301.00234.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,
Archi Mitra, Archie Sravankumar, Artem Korenev,
Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien
Rodriguez, Austen Gregerson, Ava Spataru, Bap-
tiste Rozière, Bethany Biron, Binh Tang, Bobbie
Chern, Charlotte Caucheteux, Chaya Nayak, Chloe
Bi, Chris Marra, Chris McConnell, Christian Keller,
Christophe Touret, Chunyang Wu, Corinne Wong,
Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-
lonsius, Daniel Song, Danielle Pintz, Danny Livshits,
David Esiobu, Dhruv Choudhary, Dhruv Mahajan,
Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes,
Egor Lakomkin, Ehab AlBadawy, Elina Lobanova,
Emily Dinan, Eric Michael Smith, Filip Radenovic,
Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-
gia Lewis Anderson, Graeme Nail, Grégoire Mialon,
559

Guan Pang, Guillem Cucurell, Hailey Nguyen, Han-
nah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan
Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan
Geffert, Jana Vranes, Jason Park, Jay Mahadeokar,
Jeet Shah, Jelmer van der Linde, Jennifer Billock,
Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,
Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,
Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph
Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,
Kalyan Vasuden Alwala, Kartikeya Upasani, Kate
Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and
et al. 2024. The llama 3 herd of models. arXiv
preprint arXiv:2407.21783.
Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling,
and Yongfeng Zhang. 2023. Nphardeval: Dynamic
benchmark on reasoning ability of large language
models via complexity classes. arXiv preprint
arXiv:2312.14890.
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and
Tushar Khot. 2023. Specializing smaller language
models towards multi-step reasoning. In Inter-
national Conference on Machine Learning , pages
10421–10430. PMLR.
Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and
Matt Gardner. 2020. Neural module networks for
reasoning over text. In Proceedings of the 8th In-
ternational Conference on Learning Representations
(ICLR).
Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting
Qi, Martin Riddell, Luke Benson, Lucy Sun, Eka-
terina Zubova, Yujie Qiao, Matthew Burtell, David
Peng, Jonathan Fan, Yixin Liu, Brian Wong, Mal-
colm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai,
Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fab-
bri, Wojciech Kryscinski, Xi Victoria Lin, Caiming
Xiong, and Dragomir Radev. 2022. FOLIO: natu-
ral language reasoning with first-order logic. arXiv
preprint arXiv:2209.00840.
Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin
Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and
Yanghua Xiao. 2024. Can large language models
understand real-world complex instructions? In Pro-
ceedings of the AAAI Conference on Artificial Intelli-
gence, volume 38, pages 18188–18196.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and Ja-
cob Steinhardt. 2021. Measuring mathematical prob-
lem solving with the MATH dataset. In Proceedings
of the Neural Information Processing Systems Track
on Datasets and Benchmarks (NeurIPS: Datasets
and Benchmarks).
Wenyue Hua, Kaijie Zhu, Lingyao Li, Lizhou Fan,
Shuhang Lin, Mingyu Jin, Haochen Xue, Zelong
Li, JinDong Wang, and Yongfeng Zhang. 2024. Dis-
entangling logic: The role of context in large lan-
guage model reasoning capabilities. arXiv preprint
arXiv:2406.02787.
Cheonsu Jeong. 2024. Fine-tuning and utilization
methods of domain-specific llms. arXiv preprint
arXiv:2401.02981.
Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun
Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin
Jiang, Qun Liu, and Wei Wang. 2024. Follow-
bench: A multi-level fine-grained constraints fol-
lowing benchmark for large language models. In
Proceedings of the 62nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
4667–4688.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large
language models are zero-shot reasoners. In Pro-
ceedings of the 36th Advances in Neural Information
Processing Systems (NeurIPS).
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate
Kushman, and Hannaneh Hajishirzi. 2016. MAWPS:
A math word problem repository. In Proceedings of
the 2016 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT), pages
1152–1157.
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B Hashimoto. 2023. Alpacaeval: An auto-
matic evaluator of instruction-following models.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 158–167.
Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang,
Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan
Xu, Weng Lam Tam, et al. 2023. Alignbench: Bench-
marking chinese alignment of large language models.
arXiv preprint arXiv:2311.18743.
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B.
Tenenbaum, and Jiajun Wu. 2019. The neuro-
symbolic concept learner: Interpreting scenes, words,
and sentences from natural supervision. In Proceed-
ings of the 7th International Conference on Learning
Representations (ICLR).
Norman Mu, Sarah Chen, Zifan Wang, Sizhe
Chen, David Karamardian, Lulwa Aljeraisy, Dan
Hendrycks, and David A. Wagner. 2023. Can
llms follow simple rules? arXiv preprint
arXiv:2311.04235.
OpenAI. 2023. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774.
OpenAI. 2024a. Hello gpt-4o. OpenAI Blogs.
OpenAI. 2024b. Learning to reason with llms. OpenAI
Blogs.
560

Liangming Pan, Alon Albalak, Xinyi Wang, and
William Yang Wang. 2023. Logic-lm: Empower-
ing large language models with symbolic solvers for
faithful logical reasoning. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2023,
pages 3806–3824.
Qwen Team. 2024. Qwen2.5: A party of foundation
models.
Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian
Hu, Rahul Gupta, John Frederick Wieting, Nanyun
Peng, and Xuezhe Ma. 2023. Evaluating large lan-
guage models on controlled generation tasks. arXiv
preprint arXiv:2310.14542.
Wangtao Sun, Chenxiang Zhang, Xueyou Zhang,
Ziyang Huang, Haotian Xu, Pei Chen, Shizhu He,
Jun Zhao, and Kang Liu. 2024. Beyond instruction
following: Evaluating rule following of large lan-
guage models. arXiv preprint arXiv:2407.08440.
Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.
Proofwriter: Generating implications, proofs, and
abductive statements over natural language. In Find-
ings of the Association for Computational Linguistics:
ACL/IJCNLP 2021, pages 3621–3634.
Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao,
Wangchunshu Zhou, Arman Cohan, and Mark Ger-
stein. 2023. Struc-bench: Are large language models
really good at generating complex structured data?
arXiv preprint arXiv:2309.08963.
Google Gemini Team. 2023. Gemini: A family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. arXiv
preprint arXiv:2302.13971.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2023. Interleaving retrieval
with chain-of-thought reasoning for knowledge-
intensive multi-step questions. In Proceedings of
the 61st Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 10014–10037.
Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang
Ren. 2024. Symbolic working memory enhances
language models for complex rule application. In
Proceedings of the 2024 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 17583–17604.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022. Chain-of-thought prompting
elicits reasoning in large language models. In Pro-
ceedings of the 36th Advances in Neural Information
Processing Systems (NeurIPS).
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert
Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,
Da Huang, Denny Zhou, et al. 2023. Larger language
models do in-context learning differently. arXiv
preprint arXiv:2303.03846.
Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao
Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu,
Wendy Gao, Jiaxin Xu, Yiming Liu, Jie Tang,
Hongning Wang, and Minlie Huang. 2024. Bench-
marking complex instruction-following with mul-
tiple constraints composition. arXiv preprint
arXiv:2407.03978.
Xiaobao Wu, Liangming Pan, William Yang Wang, and
Anh Tuan Luu. 2024a. AKEW: Assessing knowl-
edge editing in the wild. In Proceedings of the 2024
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 15118–15133.
Xiaobao Wu, Liangming Pan, Yuxi Xie, Ruiwen
Zhou, Shuai Zhao, Yubo Ma, Mingzhe Du, Rui
Mao, Anh Tuan Luu, and William Yang Wang.
2024b. Antileak-bench: Preventing data contam-
ination by automatically constructing benchmarks
with updated real-world knowledge. arXiv preprint
arXiv:2412.13670.
Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang,
Yihao Feng, Ran Xu, Wenpeng Yin, and Caim-
ing Xiong. 2024. Fofo: A benchmark to evaluate
llms’ format-following capability. arXiv preprint
arXiv:2402.18667.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin
Jiang. 2023. Wizardlm: Empowering large lan-
guage models to follow complex instructions. arXiv
preprint arXiv:2304.12244.
Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming
Pan, Lei Li, and William Wang. 2024. Pride and
prejudice: LLM amplifies self-bias in self-refinement.
In Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), (ACL), pages 15474–15492.
Shaokun Zhang, Xiaobo Xia, Zhaoqing Wang, Ling-
Hao Chen, Jiale Liu, Qingyun Wu, and Tongliang
Liu. 2023. Ideal: Influence-driven selective annota-
tions empower in-context learners in large language
models. arXiv preprint arXiv:2310.10873.
Xiang Zhang and Dujian Ding. 2024. Supervised chain
of thought. arXiv preprint arXiv:2410.14198.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena. In
Proceedings of the 37th Advances in Neural Informa-
tion Processing Systems (NeurIPS).
Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu,
Daya Guo, Yining Chen, Jiahai Wang, Jian Yin, Ming
561

Zhou, and Nan Duan. 2022. Analytical reasoning of
text. In Findings of the Association for Computa-
tional Linguistics: NAACL 2022, pages 2306–2319.
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-
dhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,
and Le Hou. 2023. Instruction-following evalu-
ation for large language models. arXiv preprint
arXiv:2311.07911.
Ruiwen Zhou, Yingxuan Yang, Muning Wen, Ying Wen,
Wenhao Wang, Chunling Xi, Guoqiang Xu, Yong Yu,
and Weinan Zhang. 2024. TRAD: Enhancing llm
agents with step-wise thought retrieval and aligned
decision. In Proceedings of the 47th International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval (SIGIR), pages 3–13.
Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhen-
qiang Gong, Diyi Yang, and Xing Xie. 2023. Dy-
val: Graph-informed dynamic evaluation of large
language models. arXiv preprint arXiv:2309.17167.
562

A Terminology Explanation in NBA
We briefly explain the NBA terminologies mentioned in this paper as follows:
Salary Cap. The Salary Cap of NBA is a rule that limits how much money each team can spend on
player salaries. It is designed to keep teams on a level playing field financially, so wealthier teams cannot
just purchase all the best players. The league sets the cap based on its overall revenue.
(Salary Cap) Exceptions. The NBA uses a “soft” Salary Cap, meaning teams can exceed the limit
using certain Exceptions. Following are some commonly used Exceptions:
• Mid-Level Exception (MLE) allows teams to sign free players even if they are above the salary cap.
There are three types of MLEs, i.e. Non-Taxpayer MLE, Taxpayer MLE, and MLE for Room Teams,
applicable to teams in different salary situations.
• Traded Player Exceptions (TPE) is a tool that allows teams to make trades even if they are over the
salary cap. When a team trades a player for less salary than it gives away (or for nothing), it creates a
TPE, which is like a "credit" they can use later. If a team wants to acquire more salaries than it gives
away in a trade, it can also use certain types of TPE to make such trade.
• Veteran Free Agent Exception (Bird Rights) in the NBA allow teams to re-sign their own players
even if they are over the salary cap. Named after Larry Bird, this rule encourages teams to keep their
star players. There are three types of Bird Rights, i.e. Bird Rights, Early Bird Rights, and Non-Bird
Rights, applicable to players that play for the same team for different numbers of consecutive seasons.
B Data Collection and Annotation
B.1 Rule Collection
Airline. We collect the policy for bag and optional fees from American Airlines8. Specifically, the rules
in the policy mainly include: 1) the allowance of carry-on luggage; 2) the base price for checking each
luggage on different routes and in different cabin classes; 3) the additional fees for luggage overweight or
oversize to varying degrees on different routes and in different cabin classes; 4) when calculating fees
for overweight and oversize luggage for each piece, only the higher of the two should apply. Many rules
(base price, overweight/oversize fees) in this domain are represented in tabular forms, and we regard one
entire table as one rule.
NBA. We collect the regulations for NBA transactions from2023 NBA Collective Bargaining Agree-
ments9 (CBA) and excerpt from the NBA Constitution and By-Laws 10 regarding the rules for trading
first-round draft picks (i.e., the Stepien Rule). Since the complete CBA is too long (676 Pages PDF), we
only aggregate the most commonly used rules such as the limits on salary and length of player contract, on
team salary, and on player contract trade among teams. As applying rules of the same type but applicable
under different conditions may result in completely different subsequent reasoning process, different from
in airline domain, we depart such one paragraph including such similar rules into separate rules.
Tax. We collect tax forms and relevant instructions fromInternal Revenue Service (IRS)11. Starting from
the most famous Form 1040 (U.S. Individual Income Tax Return) and its basic Schedules 1-3, we consider
more complex settings commonly happen in real-life, including using itemized deductions (Schedule A),
self-employment (Schedule C and Schedule SE), education expenses and/or credits (Form 8863), and
child and/or other dependent credits (Schedule 8812). We treat each line in these forms and its instructions
as one rule, and convert the forms into line numbers and text for each line as LLM input.
8https://www.aa.com/i18n/customer-service/support/optional-service-fees.jsp
9https://ak-static.cms.nba.com/wp-content/uploads/sites/4/2023/06/2023-NBA-Collective-Bargaining-Agreement.pdf
10https://ak-static-int.nba.com/wp-content/uploads/sites/3/2015/12/NBA-Constitution-and-By-Laws.pdf
11https://www.irs.gov/forms-instructions
563

B.2 NBA Data Annotation
For NBA tasks, we first survey famous rules and transactions that have happened in the NBA in recent
30 years and decide the 54 rules used in our annotation. To balance the task difficulty and annotation
difficulty, we further simplify the rules by unifying different types of team salary (defined in different
rules and calculated in different ways) into one simple “Team Salary”. The process of annotating one
problem is described as follows:
Creating team and player situations. Our annotators are first required to create diverse valid scenarios
involving one or more teams and players, as the following “team_situations” and “players_situations”,
and provide the number of teams (“n_teams”) and players (“n_players”) involved. Each item in the
“team_situations” list indicates the current salary of the team and its available first-round draft picks, while
each item in the“player_situations” list tells the player’s information (i.e., draft year, age, and current (or
last) contract). All players and teams are anonymized as Player (Team) A/B/C/... to avoid data leakage.
Writting transactions. Next, our annotators write “n_operations” sentences in the “operations” list,
where each item corresponding to one team signing a player or several teams conducting a trade, and
determine whether all these transactions are allowed according to the rules. The “answer” should be
True if all transactions are allowed otherwise False. If “answer” is False, we ask our annotators to
further provide “illegal_team” and “illegal_operation” as the specific team and transaction component
that violates the rules.
Listing relevant rules. Finally, our annotators are told to provide a list of “relevant_rules” including all
rules that they believe should be involved if humans need to consider the case comprehensively.
{
" n_teams " : int = ... ,
" n_players " : int = ... ,
" n_operations " : int = ... ,
" team_situations " : list [ str ] = [...] ,
" pla ye r_ si tua ti on s " : list [ str ] = [...] ,
" operations " : list [ str ] = [...] ,
" answer " : bool = ... ,
" ill eg al _o per at io n " : str = ... ,
" illegal_team " : str = ... ,
" relevant_rules " : list [ str ] = [...]
}
The format of annotated NBA test problems.
To ensure the quality of annotation, we provide each annotator with a detailed annotation document
and training sessions, and ask our annotators to annotate a small subset of problems and give explanations
for verification before annotation. Only if each instance in the verification subset is correct, the annotator
will be invited for the formal annotation.
C Structured Rule Extraction in Each Scenario
As introduced in Section 4.1, we utilize the structured output mode of GPT-4o (OpenAI, 2024a) to convert
LLMs’ textual output into structured data. Here we present the data structure we used in parsing.
Airline. In airline domain we ask LLMs to parse the the list of checked luggage as well as provided
basic information.
class BagCost ( BaseModel ) :
size : int
weight : int
base_check_fee : int
oversize_fee : int
overweight_fee : int
total_fee : int
class PassengerClass ( str , Enum ) :
be = " Basic Economy "
main = " Main Cabin "
mp = " Main Plus "
pe = " Premium Economy "
business = " Business "
first = " First "
class Response ( BaseModel ) :
passenger_class : str
p la c e _o f _ de p a rt u r e : str
plac e_of_arri val : str
ticket_price : int
checked_bags : list [ BagCost ]
total_cost : int
564

NBA. In NBA domain we let the LLM parser decide whether each of the 54 rules is applied.
class RuleExtraction ( BaseModel ) :
# contract length
c o n t r a c t _ l e n g t h _ a t _ m o s t _ 4 _ y e a r _ e x c e p t _ q u a l i f y i n g _ v e t e r a n _ f r e e _ a g e n t _ 5 _ y e a r : bool
c o n t r a c t _ l e n g t h _ a t _ m o s t _ 2 _ y e a r _ b i _ a n n u a l _ e x c e p t i o n : bool
c o n t r a c t _ l e n g t h _ a t _ m o s t _ 4 _ y e a r _ n o n _ t a x p a y e r _ m i d _ l e v e l _ e x c e p t i o n : bool
c o n t r a c t _ l e n g t h _ a t _ m o s t _ 2 _ y e a r _ t a x p a y e r _ m i d _ l e v e l _ e x c e p t i o n : bool
c o n t r a c t _ l e n g t h _ a t _ m o s t _ 3 _ y e a r _ m i d _ l e v e l _ e x c e p t i o n _ f o r _ r o o m _ t e a m : bool
c o n t r a c t _ l e n g t h _ a t _ m o s t _ 2 _ y e a r _ m i n i m u m _ p l a y e r _ s a l a r y _ e x c e p t i o n : bool
# basic rules
s a l a r y _ c a p _ n o _ e x c e e d _ w i t h o u t _ e x c e p t i o n : bool
m a x i m u m _ s a l a r y _ f o r _ p l a y e r _ l e s s _ t h a n _ 7 _ y e a r _ s e r v i c e : bool
m a x i m u m _ s a l a r y _ f o r _ p l a y e r _ 7 _ t o _ 9 _ y e a r _ s e r v i c e : bool
m a x i m u m _ s a l a r y _ f o r _ p l a y e r _ 1 0 _ o r _ m o r e _ y e a r _ s e r v i c e : bool
h i g h e r _ m a x _ c r i t e r i o n _ f o r _ 5 t h _ y e a r _ e l i g i b l e _ p l a y e r : bool
s a l a r y _ i n c r e a s e _ a n d _ d e c r e a s e _ r a t i o _ e x c e p t _ q u a l i y f i n g _ o r _ e a r l y _ q u a l i f y i n g _ v e t e r a n _ f r e e _ a g e n t : bool
s a l a r y _ i n c r e a s e _ a n d _ d e c r e a s e _ r a t i o _ f o r _ q u a l i y f i n g _ o r _ e a r l y _ q u a l i f y i n g _ v e t e r a n _ f r e e _ a g e n t : bool
# 38 year old provision
d e f e r _ c o m p e n s a t i o n _ 3 8 _ y e a r _ o l d : bool
d e f e r _ c o m p e n s a t i o n _ q u a l i f y i n g _ v e t e r a n _ f r e e _ a g e n t _ 3 8 _ y e a r _ o l d : bool
# apron level as hard cap rules
b i _ a n n u a l _ e x c e p t i o n _ h a r d _ c a p _ f i r s t _ a p r o n _ l e v e l : bool
n o n _ t a x p a y e r _ m i d _ l e v e l _ e x c e p t i o n _ h a r d _ c a p _ f i r s t _ a p r o n _ l e v e l : bool
s i g n _ a n d _ t r a d e _ h a r d _ c a p _ f i r s t _ a p r o n _ l e v e l : bool
e x p a n d e d _ t r a d e d _ p l a y e r _ e x c e p t i o n _ h a r d _ c a p _ f i r s t _ a p r o n _ l e v e l : bool
a g g r e g a t e d _ t r a d e d _ p l a y e r _ e x c e p t i o n _ h a r d _ c a p _ s e c o n d _ a p r o n _ l e v e l : bool
c a s h _ i n _ t r a d e _ h a r d _ c a p _ s e c o n d _ a p r o n _ l e v e l : bool
s i g n _ a n d _ t r a d e _ a s s i g n e r _ t r a d e d _ p l a y e r _ e x c e p t i o n _ h a r d _ c a p _ s e c o n d _ a p r o n _ l e v e l : bool
t a x p a y e r _ m i d _ l e v e l _ e x c e p t i o n _ h a r d _ c a p _ s e c o n d _ a p r o n _ l e v e l : bool
t r a d e d _ p l a y e r _ e x c e p t i o n _ 2 5 0 k _ r e d u c e d _ f i r s t _ a p r o n _ l e v e l : bool
# exceptions
# bird rights
q u a l i f y i n g _ v e t e r a n _ f r e e _ a g e n t _ e x c e p t i o n : bool
e a r l y _ q u a l i f y i n g _ v e t e r a n _ f r e e _ a g e n t _ e x c e p t i o n : bool
n o n _ q u a l i f y i n g _ v e t e r a n _ f r e e _ a g e n t _ e x c e p t i o n : bool
s a l a r y _ s p a c e _ c o n s u m p t i o n _ q u a l i f y i n g _ v e t e r a n _ f r e e _ a g e n t : bool
s a l a r y _ s p a c e _ c o n s u m p t i o n _ e a r l y _ q u a l i f y i n g _ v e t e r a n _ f r e e _ a g e n t : bool
s a l a r y _ s p a c e _ c o n s u m p t i o n _ n o n _ q u a l i f y i n g _ v e t e r a n _ f r e e _ a g e n t : bool
s a l a r y _ s p a c e _ c o n s u m p t i o n _ s t a n d a r d _ t r a d e d _ p l a y e r _ e x c e p t i o n : bool
# bi - annual exception
b i _ a n n u a l _ e x c e p t i o n : bool
# mid level exceptions
n o n _ t a x p a y e r _ m i d _ l e v e l _ e x c e p t i o n : bool
t a x p a y e r _ m i d _ l e v e l _ e x c e p t i o n : bool
m i d _ l e v e l _ e x c e p t i o n _ f o r _ r o o m _ t e a m : bool
m i n i m u m _ p l a y e r _ s a l a r y _ e x c e p t i o n : bool
# traded player exceptions
s t a n d a r d _ t r a d e d _ p l a y e r _ e x c e p t i o n : bool
a g g r e g a t e d _ s t a n d a r d _ t r a d e d _ p l a y e r _ e x c e p t i o n : bool
e x p a n d e d _ t r a d e d _ p l a y e r _ e x c e p t i o n : bool
t r a d e d _ p l a y e r _ e x c e p t i o n _ f o r _ r o o m _ t e a m : bool
t r a d e d _ p l a y e r _ e x c e p t i o n _ o n l y _ o n e _ m i n i m u m _ t r a d e d _ p l a y e r _ u n d e r _ c o n d i t i o n s : bool
# trade rules
p a y _ o r _ r e c e i v e _ c a s h _ m a x i m u m _ i n _ a _ y e a r : bool
r o o k i e _ o r _ t w o _ w a y _ c o n t r a c t _ c a n n o t _ b e _ t r a d e d _ w i t h i n _ 3 0 _ d a y s : bool
f r e e _ a g e n t _ s i g n _ c o n t r a c t _ c a n n o t _ b e _ t r a d e d _ w i t h i n _ 3 _ m o n t h _ o r _ b e f o r e _ d e c _ 1 5 : bool
q u a l i f y i n g _ o r _ e a r l y _ q u a l i f y i n g _ f r e e _ a g e n t _ s i g n _ c o n t r a c t _ c a n n o t _ b e _ t r a d e d _ w i t h i n _ 3 _ m o n t h _ o r _ b e f o r e _ j a n _ 1 5 : bool
# sign - and - trade rules
s i g n _ a n d _ t r a d e _ 3 _ t o _ 4 _ y e a r : bool
s i g n _ a n d _ t r a d e _ n o t _ w i t h _ m i d _ l e v e l _ e x c e p t i o n : bool
s i g n _ a n d _ t r a d e _ n o _ h i g h e r _ t h a n _ 2 5 _ p e r c e n t _ f o r _ h i g h e r _ m a x _ 5 t h _ y e a r _ e l i g i b l e _ p l a y e r : bool
s i g n _ a n d _ t r a d e _ a s s i g n e e _ t e a m _ h a s _ r o o m : bool
s i g n _ a n d _ t r a d e _ q u a l i f y i n g _ f r e e _ a g e n t _ h a l f _ s a l a r y _ f o r _ t r a d e d _ p l a y e r _ e x c e p t i o n : bool
# restricted free agent rules ( Arenas provision )
o f f e r _ s h e e t _ f o r _ 1 _ o r _ 2 _ y e a r _ s e r v i c e _ p l a y e r _ n o _ m o r e _ t h a n _ m i d _ l e v e l _ i n _ f i r s t _ 2 _ y e a r : bool
o f f e r _ s h e e t _ f o r _ 1 _ o r _ 2 _ y e a r _ s e r v i c e _ p l a y e r _ 3 r d _ y e a r _ m a x i m u m _ i f _ f i r s t _ 2 _ y e a r _ m a x i m u m : bool
o f f e r _ s h e e t _ f o r _ 1 _ o r _ 2 _ y e a r _ s e r v i c e _ p l a y e r _ 4 t h _ y e a r _ m a x i m u m _ i f _ 3 _ y e a r : bool
o f f e r _ s h e e t _ f o r _ 1 _ o r _ 2 _ y e a r _ s e r v i c e _ p l a y e r _ a v e r a g e _ s a l a r y _ m o r e _ t h a n _ 2 _ y e a r : bool
# first - round draft pick trade rules
s t e p i e n _ r u l e _ n o _ s e l l _ o r _ n o _ c o n s e c u t i v e _ f i r s t _ r o u n d _ d r a f t _ p i c k _ t r a d e : bool
Tax. In tax domain we just list each line in Form 1040 and its Schedules 1-3 for parsing.
class Form1040 ( BaseModel ) :
name : str = Field ( description = " Name of taxpayer " )
age : int = Field ( description = " Age of taxpayer " )
spouse_age : int = Field ( description = " Age of taxpayer 's spouse " )
filing_status : FilingStatus = Field ( description = " Filing status of taxpayer " )
blind : bool = Field ( description = " Taxpayer is blind " )
spouse_blind : bool = Field ( description = " Taxpayer 's spouse is blind " )
itemized : bool = Field ( description = " Taxpayer uses itemized deductions " )
n u m _ q u a l i f y i n g _ c h i l d r e n : int = Field ( description = " Number of qualifying children " )
n u m _ o t h e r _ d e p e n d e n t s : int = Field ( description = " Number of other dependents " )
w a g e _ t i p _ c o m p e n s a t i o n : float = Field ( description = " Form 1040 Line 1 a " )
h o u s e h o l d _ e m p l o y e e _ w a g e : float = Field ( description = " Form 1040 Line 1 b " )
unreported_tip : float = Field ( description = " Form 1040 Line 1 c " )
565

n o n t a x a b l e _ c o m b a t _ p a y : float = Field ( description = " Form 1040 Line 1 d " )
w a g e _ t i p _ c o m p e n s a t i o n _ t o t a l : float = Field ( description = " Form 1040 Line 1 z " )
t a x _ e x e m p t _ i n t e r e s t : float = Field ( description = " Form 1040 Line 2 a " )
taxa ble_inter est : float = Field ( description = " Form 1040 Line 2 b " )
q u a l i f i e d _ d i v i d e n d s : float = Field ( description = " Form 1040 Line 3 a " )
o rd i n ar y _ di v i de n d s : float = Field ( description = " Form 1040 Line 3 b " )
ir a_ di st ri but io ns : float = Field ( description = " Form 1040 Line 4 a " )
t a x a b l e _ i r a _ d i s t r i b u t i o n s : float = Field ( description = " Form 1040 Line 4 b " )
all_pensions : float = Field ( description = " Form 1040 Line 5 a " )
taxa ble_pensi ons : float = Field ( description = " Form 1040 Line 5 b " )
s o c i a l _ s e c u r i t y _ b e n e f i t s : float = Field ( description = " Form 1040 Line 6 a " )
t a x a b l e _ s o c i a l _ s e c u r i t y _ b e n e f i t s : float = Field ( description = " Form 1040 Line 6 b " )
c a p i t a l _ g a i n _ o r _ l o s s : float = Field ( description = " Form 1040 Line 7 " )
ad di ti on al _in co me : float = Field ( description = " Form 1040 Line 8 " )
total_income : float = Field ( description = " Form 1040 Line 9 " )
to ta l_ ad ju stm en ts : float = Field ( description = " Form 1040 Line 10 " )
a d j u s t e d _ g r o s s _ i n c o m e : float = Field ( description = " Form 1040 Line 11 " )
s t a n d a r d _ o r _ i t e m i z e d _ d e d u c t i o n s : float = Field ( description = " Form 1040 Line 12 " )
q u a l i f i e d _ b u s i n e s s _ i n c o m e : float = Field ( description = " Form 1040 Line 13 " )
tota l_deducti ons : float = Field ( description = " Form 1040 Line 14 " )
c o m p u t e d _ t a x a b l e _ i n c o m e : float = Field ( description = " Form 1040 Line 15 " )
taxes : float = Field ( description = " Form 1040 Line 16 " )
c o p y _ s c h e d u l e _ 2 _ l i n e _ 3 : float = Field ( description = " Form 1040 Line 17 " )
f1040_line_18 : float = Field ( description = " Form 1040 Line 18 " )
c t c _ o r _ o t h e r _ d e p e n d e n t _ c r e d i t : float = Field ( description = " Form 1040 Line 19 " )
c o p y _ s c h e d u l e _ 3 _ l i n e _ 8 : float = Field ( description = " Form 1040 Line 20 " )
a c c u m u l a t e d _ c r e d i t s : float = Field ( description = " Form 1040 Line 21 " )
t a x e s _ a f t e r _ c r e d i t s : float = Field ( description = " Form 1040 Line 22 " )
other_taxes : float = Field ( description = " Form 1040 Line 23 " )
total_tax : float = Field ( description = " Form 1040 Line 24 " )
f e d e r a l _ i n c o m e _ t a x _ w i t h h e l d : float = Field ( description = " Form 1040 Line 25 " )
e a r n e d _ i n c o m e _ c r e d i t : float = Field ( description = " Form 1040 Line 27 " )
a d d i t i o n a l _ c h i l d _ t a x _ c r e d i t : float = Field ( description = " Form 1040 Line 28 " )
a m e r i c a n _ o p p o r t u n i t y _ c r e d i t : float = Field ( description = " Form 1040 Line 29 " )
c o p y _ s c h e d u l e _ 3 _ l i n e _ 1 5 : float = Field ( description = " Form 1040 Line 31 " )
t o t a l _ o t h e r _ p a y m e n t s _ a n d _ r e f u n d a b l e _ c r e d i t s : float = Field ( description = " Form 1040 Line 32 " )
total_payments : float = Field ( description = " Form 1040 Line 33 " )
a m o u n t _ o w e d _ o r _ o v e r p a i d : float = Field ( description = " Form 1040 Line 37 ( negative if overpaid ) " )
t a x a b l e _ s t a t e _ r e f u n d s : float = Field ( description = " Schedule 1 Line 1 " )
alimony_income : float = Field ( description = " Schedule 1 Line 2 a " )
sale _of_busin ess : float = Field ( description = " Schedule 1 Line 4 " )
r e n t a l _ r e a l _ e s t a t e _ s c h 1 : float = Field ( description = " Schedule 1 Line 5 " )
farm_income : float = Field ( description = " Schedule 1 Line 6 " )
u n e m p l o y m e n t _ c o m p e n s a t i o n : float = Field ( description = " Schedule 1 Line 7 " )
other_income : float = Field ( description = " Schedule 1 Line 8 " )
ed uc at or _e xpe ns es : float = Field ( description = " Schedule 1 Line 11 " )
hsa_deduction : float = Field ( description = " Schedule 1 Line 13 " )
s e l f _ e m p l o y m e n t _ d e d u c t i b l e : float = Field ( description = " Schedule 1 Line 15 " )
ira_deduction : float = Field ( description = " Schedule 1 Line 20 " )
s t u d e n t _ l o a n _ i n t e r e s t _ d e d u c t i o n : float = Field ( description = " Schedule 1 Line 21 " )
ot he r_ ad ju stm en ts : float = Field ( description = " Schedule 1 Line 24 " )
amt_f6251 : float = Field ( description = " Schedule 2 Line 1 " )
cred it_repaym ent : float = Field ( description = " Schedule 2 Line 2 " )
s c h e d u l e _ 2 _ t o t a l _ t a x e s : float = Field ( description = " Schedule 2 Line 3 (= Line 1 + Line 2) " )
s e l f _ e m p l o y m e n t _ t a x : float = Field ( description = " Schedule 2 Line 4 " )
o t h e r _ a d d i t i o n a l _ t a x e s : float = Field ( description = " Schedule 2 Line 17 " )
s c h e d u l e _ 2 _ t o t a l _ o t h e r _ t a x e s : float = Field ( description = " Schedule 2 Line 21 (= Line 4 + Line 17) " )
f or e i gn _ t ax _ c re d i t : float = Field ( description = " Schedule 3 Line 1 " )
dependent_care : float = Field ( description = " Schedule 3 Line 2 " )
c o m p u t e d _ e d u c a t i o n _ c r e d i t s : float = Field ( description = " Schedule 3 Line 3 " )
r et i r em e n t_ s a vi n g s : float = Field ( description = " Schedule 3 Line 4 " )
e l d e r l y _ d i s a b l e d _ c r e d i t s : float = Field ( description = " Schedule 3 Line 6 d " )
p l u g _ i n _ m o t o r _ v e h i c l e : float = Field ( description = " Schedule 3 Line 6 i " )
al t_ mo to r_ veh ic le : float = Field ( description = " Schedule 3 Line 6 j " )
sc he du le _3 _li ne _8 : float = Field ( description = " Schedule 3 Line 8 " )
m e d i c a l _ d e n t a l _ e x p e n s e s : Optional [ float ] = Field ( description = " Schedule A Line 1 ( if itemized ) " )
s t a t e _ l o c a l _ i n c o m e _ o r _ s a l e s _ t a x : Optional [ float ] = Field ( description = " Schedule A Line 5 a ( if itemized ) " )
s t a t e _ l o c a l _ r e a l _ e s t a t e _ t a x : Optional [ float ] = Field ( description = " Schedule A Line 5 b ( if itemized ) " )
s t a t e _ l o c a l _ p e r s o n a l _ p r o p e r t y _ t a x : Optional [ float ] = Field ( description = " Schedule A Line 5 c ( if itemized ) " )
othe r_taxes_p aid : Optional [ float ] = Field ( description = " Schedule A Line 6 ( if itemized ) " )
h o m e _ m o r t g a g e _ i n t e r e s t _ a n d _ p o i n t s : Optional [ float ] = Field ( description = " Schedule A Line 8 a ( if itemized ) " )
h o m e _ m o r t g a g e _ i n t e r e s t _ u n r e p o r t e d : Optional [ float ] = Field ( description = " Schedule A Line 8 b " )
h o m e _ m o r t g a g e _ p o i n t s _ u n r e p o r t e d : Optional [ float ] = Field ( description = " Schedule A Line 8 c ( if itemized ) " )
i n v e s t m e n t _ i n t e r e s t : Optional [ float ] = Field ( description = " Schedule A Line 9 ( if itemized ) " )
charity_cash : Optional [ float ] = Field ( description = " Schedule A Line 11 ( if itemized ) " )
char ity_non_c ash : Optional [ float ] = Field ( description = " Schedule A Line 12 ( if itemized ) " )
c a s u a l t y _ a n d _ t h e f t _ l o s s : Optional [ float ] = Field ( description = " Schedule A Line 15 ( if itemized ) " )
o t h e r _ i t e m i z e d _ d e d u c t i o n s : Optional [ float ] = Field ( description = " Schedule A Line 16 ( if itemized ) " )
gross_receipts : Optional [ float ] = Field ( description = " Schedule C Line 1 ( if self - employed ) " )
r e t u r n s _ a n d _ a l l o w a n c e s : Optional [ float ] = Field ( description = " Schedule C Line 2 ( if self - employed ) " )
c os t _ of _ g oo d s _s o l d : Optional [ float ] = Field ( description = " Schedule C Line 4 ( if self - employed ) " )
ot he r_ in c_ sch ed _c : Optional [ float ] = Field ( description = " Schedule C Line 6 ( if self - employed ) " )
total_expenses : Optional [ float ] = Field ( description = " Schedule C Line 28 ( if self - employed ) " )
expe nses_of_h ome : Optional [ float ] = Field ( description = " Schedule C Line 30 ( if self - employed ) " )
net_profit : Optional [ float ] = Field ( description = " Schedule C Line 31 ( if self - employed ) " )
t o t a l _ s o c i a l _ s e c u r i t y _ w a g e s : Optional [ float ] = Field ( description = " Schedule SE Line 8 ( if self - employed ) " )
student_list : Optional [ list [ Student ]] = Field ( description = " List of students with education expenses " )
D More Experiment Results and Analysis
D.1 Rule-Wise Statistics
We visualize the rule-wise recall, precision, and correctness in Figure 4-6. Since precision is always 1.0
in airline and tax domains, we skip these two charts.
566

0.0 0.2 0.4 0.6 0.8 1.0
Recall
Rule 1
Rule 2
Rule 3
Rule 4
Rule 5
Rule 6
Rule 7
Rule 8
Rule 9
Rule 10Rule
Rule 1 -- overall fee aggregation
Rule 2 -- 1st check bag fee
Rule 3 -- 2nd check bag fee
Rule 4 -- overweight fee
Rule 5 -- 4+ check bag fee
Rule 6 -- main plus extra free bag
Rule 7 -- 3rd check bag fee
Rule 8 -- oversize fee
Rule 9 -- complementary overweight
Rule 10 -- maximum violation fee
(a) Recall
0.0 0.2 0.4 0.6 0.8 1.0
Precision
Rule 1
Rule 2
Rule 3
Rule 4
Rule 5
Rule 6
Rule 7
Rule 8
Rule 9
Rule 10Rule
Rule 1 -- complementary overweight
Rule 2 -- maximum violation fee
Rule 3 -- 4+ check bag fee
Rule 4 -- 1st check bag fee
Rule 5 -- 2nd check bag fee
Rule 6 -- oversize fee
Rule 7 -- 3rd check bag fee
Rule 8 -- overweight fee
Rule 9 -- overall fee aggregation
Rule 10 -- main plus extra free bag (b) Correctness
Figure 4: Rule-wise metrics of rules in airline domain.
0.0 0.2 0.4 0.6 0.8 1.0
Recall
Rule 1
Rule 2
Rule 3
Rule 4
Rule 5
Rule 6
Rule 7
Rule 8
Rule 9
Rule 10
Rule 11
Rule 12
Rule 13
Rule 14
Rule 15
Rule 16
Rule 17
Rule 18
Rule 19
Rule 20
Rule 21
Rule 22
Rule 23
Rule 24
Rule 25
Rule 26
Rule 27
Rule 28
Rule 29
Rule 30
Rule 31
Rule 32
Rule 33
Rule 34
Rule 35
Rule 36
Rule 37
Rule 38
Rule 39
Rule 40
Rule 41
Rule 42
Rule 43
Rule 44
Rule 45
Rule 46
Rule 47
Rule 48
Rule 49
Rule 50
Rule 51
Rule 52Rule
Rule 1 -- minimum salary exception
Rule 2 -- contract length minimum salary
Rule 3 -- higher max criterion
Rule 4 -- salary cap no exceed
Rule 5 -- salary increase except (early) bird rights
Rule 6 -- stepien rule
Rule 7 -- pay or receive cash max
Rule 8 -- arenas provision maximum first 2 year
Rule 9 -- trade time limit new free agent
Rule 10 -- arenas provision 3rd year
Rule 11 -- taxpayer mid level exception
Rule 12 -- standard traded player exception
Rule 13 -- bird rights
Rule 14 -- contract length standard
Rule 15 -- contract length taxpayer mid level exception
Rule 16 -- non-taxpayer mid level exception
Rule 17 -- bi-annual exception hard cap
Rule 18 -- traded player exception for room team
Rule 19 -- taxpayer mid level exception hard cap
Rule 20 -- non-taxpayer mid level exception hard cap
Rule 21 -- salary increase for (early) bird rights
Rule 22 -- contract length sign and trade
Rule 23 -- maximum salary 7 to 9
Rule 24 -- non bird rights
Rule 25 -- maximum salary 10 or more
Rule 26 -- sign and trade assignee has room
Rule 27 -- bi-annual exception
Rule 28 -- traded player exception no 250k first apron level
Rule 29 -- contract length non-taxpayer mid level exception
Rule 30 -- sign and trade not with mid level exception
Rule 31 -- sign and trade assignee hard cap
Rule 32 -- cash in trade hard cap
Rule 33 -- arenas provision offer sheet average salary
Rule 34 -- mid level exception for room team
Rule 35 -- maximum salary less than 7
Rule 36 -- expanded traded player exception
Rule 37 -- contract length mid level exception for room team
Rule 38 -- aggregated standard traded player exception
Rule 39 -- early bird rights
Rule 40 -- contract length bi-annual exception
Rule 41 -- aggregated standard traded player exception hard cap
Rule 42 -- expanded traded player exception hard cap
Rule 43 -- sign and trade assigner hard cap
Rule 44 -- minimum salary aggregation limit
Rule 45 -- salary space consumption of bird rights
Rule 46 -- sign and trade (early) bird rights change salary
Rule 47 -- salary space consumption of non bird rights
Rule 48 -- salary space consumption of early bird rights
Rule 49 -- maximum salary sign and trade higher max
Rule 50 -- arenas provision 4th year
Rule 51 -- over 38 rule
Rule 52 -- over 38 rule with bird rights
(a) Recall
567

0.0 0.2 0.4 0.6 0.8 1.0
Precision
Rule 1
Rule 2
Rule 3
Rule 4
Rule 5
Rule 6
Rule 7
Rule 8
Rule 9
Rule 10
Rule 11
Rule 12
Rule 13
Rule 14
Rule 15
Rule 16
Rule 17
Rule 18
Rule 19
Rule 20
Rule 21
Rule 22
Rule 23
Rule 24
Rule 25
Rule 26
Rule 27
Rule 28
Rule 29
Rule 30
Rule 31
Rule 32
Rule 33
Rule 34
Rule 35
Rule 36
Rule 37
Rule 38
Rule 39
Rule 40
Rule 41
Rule 42
Rule 43
Rule 44
Rule 45
Rule 46
Rule 47
Rule 48
Rule 49
Rule 50Rule
Rule 1 -- contract length sign and trade
Rule 2 -- sign and trade assignee hard cap
Rule 3 -- sign and trade not with mid level exception
Rule 4 -- salary cap no exceed
Rule 5 -- sign and trade assignee has room
Rule 6 -- arenas provision 3rd year
Rule 7 -- maximum salary 7 to 9
Rule 8 -- arenas provision offer sheet average salary
Rule 9 -- pay or receive cash max
Rule 10 -- stepien rule
Rule 11 -- sign and trade (early) bird rights change salary
Rule 12 -- salary increase for (early) bird rights
Rule 13 -- arenas provision maximum first 2 year
Rule 14 -- expanded traded player exception
Rule 15 -- contract length minimum salary
Rule 16 -- cash in trade hard cap
Rule 17 -- sign and trade assigner hard cap
Rule 18 -- maximum salary 10 or more
Rule 19 -- contract length mid level exception for room team
Rule 20 -- non-taxpayer mid level exception hard cap
Rule 21 -- maximum salary less than 7
Rule 22 -- contract length non-taxpayer mid level exception
Rule 23 -- bird rights
Rule 24 -- aggregated standard traded player exception
Rule 25 -- non-taxpayer mid level exception
Rule 26 -- contract length standard
Rule 27 -- taxpayer mid level exception
Rule 28 -- minimum salary aggregation limit
Rule 29 -- minimum salary exception
Rule 30 -- contract length taxpayer mid level exception
Rule 31 -- expanded traded player exception hard cap
Rule 32 -- salary increase except (early) bird rights
Rule 33 -- early bird rights
Rule 34 -- standard traded player exception
Rule 35 -- taxpayer mid level exception hard cap
Rule 36 -- trade time limit new free agent
Rule 37 -- aggregated standard traded player exception hard cap
Rule 38 -- mid level exception for room team
Rule 39 -- bi-annual exception
Rule 40 -- bi-annual exception hard cap
Rule 41 -- contract length bi-annual exception
Rule 42 -- higher max criterion
Rule 43 -- non bird rights
Rule 44 -- traded player exception for room team
Rule 45 -- traded player exception no 250k first apron level
Rule 46 -- salary space consumption of bird rights
Rule 47 -- salary space consumption of non bird rights
Rule 48 -- arenas provision 4th year
Rule 49 -- maximum salary sign and trade higher max
Rule 50 -- salary space consumption of early bird rights
(b) Precision
Figure 5: Rule-wise metrics of rules in NBA domain.
0.0 0.2 0.4 0.6 0.8 1.0
Recall
Rule 1
Rule 2
Rule 3
Rule 4
Rule 5
Rule 6
Rule 7
Rule 8
Rule 9
Rule 10
Rule 11
Rule 12
Rule 13
Rule 14
Rule 15
Rule 16
Rule 17
Rule 18
Rule 19
Rule 20
Rule 21
Rule 22
Rule 23
Rule 24
Rule 25
Rule 26
Rule 27
Rule 28
Rule 29
Rule 30
Rule 31Rule
Rule 1 -- wage and tip compensation
Rule 2 -- standard deductions
Rule 3 -- adjusted gross income
Rule 4 -- schedule 2 part i taxes copy
Rule 5 -- total income
Rule 6 -- accumulated credits
Rule 7 -- self employment deductible
Rule 8 -- payments and refundable credits
Rule 9 -- taxes after credits
Rule 10 -- total payments
Rule 11 -- accumulated taxes
Rule 12 -- amount owed or overpaid
Rule 13 -- taxable income
Rule 14 -- total deductions
Rule 15 -- schedule 2 part i taxes
Rule 16 -- total adjustments
Rule 17 -- standard taxes
Rule 18 -- total taxes
Rule 19 -- additional income
Rule 20 -- total other taxes
Rule 21 -- total other taxes copy
Rule 22 -- schedule 3 total credits
Rule 23 -- itemized deductions
Rule 24 -- self employment tax
Rule 25 -- additional child tax credit
Rule 26 -- schedule 3 line 8
Rule 27 -- taxes with qualified dividends
Rule 28 -- ctc or other dependent credit
Rule 29 -- net profit
Rule 30 -- american opportunity credit
Rule 31 -- education credits
(a) Recall
0.0 0.2 0.4 0.6 0.8 1.0
Precision
Rule 1
Rule 2
Rule 3
Rule 4
Rule 5
Rule 6
Rule 7
Rule 8
Rule 9
Rule 10
Rule 11
Rule 12
Rule 13
Rule 14
Rule 15
Rule 16
Rule 17
Rule 18
Rule 19
Rule 20
Rule 21
Rule 22
Rule 23
Rule 24
Rule 25
Rule 26
Rule 27
Rule 28
Rule 29
Rule 30
Rule 31Rule
Rule 1 -- schedule 2 part i taxes copy
Rule 2 -- total payments
Rule 3 -- schedule 2 part i taxes
Rule 4 -- schedule 3 total credits
Rule 5 -- accumulated credits
Rule 6 -- accumulated taxes
Rule 7 -- payments and refundable credits
Rule 8 -- total deductions
Rule 9 -- taxable income
Rule 10 -- wage and tip compensation
Rule 11 -- taxes after credits
Rule 12 -- total other taxes
Rule 13 -- additional child tax credit
Rule 14 -- adjusted gross income
Rule 15 -- total taxes
Rule 16 -- total other taxes copy
Rule 17 -- amount owed or overpaid
Rule 18 -- self employment deductible
Rule 19 -- american opportunity credit
Rule 20 -- education credits
Rule 21 -- ctc or other dependent credit
Rule 22 -- self employment tax
Rule 23 -- net profit
Rule 24 -- total adjustments
Rule 25 -- schedule 3 line 8
Rule 26 -- additional income
Rule 27 -- total income
Rule 28 -- standard deductions
Rule 29 -- itemized deductions
Rule 30 -- standard taxes
Rule 31 -- taxes with qualified dividends (b) Correctness
Figure 6: Rule-wise metrics of rules in tax domain.
568

D.2 What Impacts Rule Following?
In this section, we investigate the factors influencing LLM performance, as measured by Acc(t). We
begin by examining the correlation between Acc(t) and other key metrics, including P(t), AC(t), and
R(t). We then consider the effects of in-context examples, different rule representations, and the presence
of distractors.
D.2.1 Correlation Between Accuracy and Other Metrics
To understand which factors most directly affect Acc(t), we visualize its correlation with other metrics in
Figure 7 across all three domains on datapoints from all difficulty levels. From Figure 7a and Figure 7b,
we observe an almost linear relationship between R(t) and Acc(t). Notice that in the tax domain (Figure
7c), a recall lower than 0.95 immediately results in zero accuracy.
In contrast, the correlation between AC(t) and Acc(t) is highly non-linear, as seen in Figure 7a and
Figure 7c. In many cases, a single computational error in rule application (thus reducing AC(t)) is
sufficient to produce an incorrect final answer, indicating that only near-perfect AC(t) leads to significant
Acc(t) improvements. For the NBA domain, we also compare P(t) and Acc(t); since P(t) is always
100% for the airline and tax domains, these correlations are not meaningful there. We find no clear
relationship between P(t) and Acc(t) for the NBA problems (Figure 7b).
0.0-0.60.6-0.70.7-0.80.8-0.90.9-0.950.95-1.0
Recall/Correctness Range
0.00
0.05
0.10
0.15
0.20
0.25Accuracy
Recall
Correctness
(a) Airline
0.0-0.4 0.4-0.6 0.6-0.8 0.8-1.0
Recall/Precision Range
0.2
0.3
0.4
0.5
0.6Accuracy
Recall
Precision (b) NBA
0.0-0.85 0.85-0.9 0.9-0.95 0.95-1.0
Recall/Correctness Range
0.00
0.02
0.04
0.06
0.08
0.10Accuracy
Recall
Correctness (c) Tax
Figure 7: Correlation between problem-wise metrics and accuracy. The correlation is the most obvious and almost
linear between R(t) and Acc(t), while highly non-linear or unclear between other two metrics and Acc(t).
D.2.2 Do In-Context Examples Help?
Table 3 presents the results with or without a level-1 1-shot example. LLMs generally provide better
performances given 1-shot example on airline, tax, and (easy) NBA problems. Many studies have shown
the benefit of in-context learning (Dong et al., 2022; Wei et al., 2023; Zhang et al., 2023), which conforms
with our observation that Acc(t) gets higher in the 1-shot setting. This performance boost comes from
both the enhancement of AC(t) as well as a better understanding of the reasoning process, indicated by
higher R(t).
However, when tackling more challenging NBA problems (Levels 2 and 3), providing an example
increases P(t) and R(t) but leads to a counterintuitive decrease in overall Acc(t). This improvement in
precision and recall primarily arises from the non-essential rules included in the in-context example, such
as the “Over 38 rule” and “Salary consumption of veteran free agent”. We compute theR(r) and P(r) for
these two rules as in Table 8.
Notably, while the R(r) for both rules improves, P(r) for rule “Salary consumption” is much lower.
This shows that thought the in-context example does remind LLMs to apply rules that they might
overlook, some rules like “Salary consumption” can be too hard for LLMs to understand even taught
by an expert example, and thus LLMs do not understand what scenarios are suitable for such rules to
apply. In addition, we find the performance on the remaining rules remains mostly unchanged. The exact
cause of the performance decline in accuracy is difficult to pinpoint as our annotation on NBA does not
contain detailed intermediate reasoning annotations. However, prior work (Fan et al., 2023) suggests
569

Rule Setting R(r) P( r)
Over 38 rule 0-shot 0.00 N/A
1-shot 0.35 0.76
Salary consumption 0-shot 0.00 N/A
1-shot 0.23 0.20
Table 8: 0-shot and 1-shot rule-wise comparison.
that if the in-context example is “easier” than the target problem, the example can inadvertently degrade
performance—a plausible explanation for why accuracy drops even as precision and recall improve.
D.2.3 Does Rule Representation Matter?
In the airline and tax domains, some rules are represented as Markdown tables. To test whether repre-
sentation format affects performance, we convert these tabular rules into textual “if-then” statements.
Table 9 shows that converting tabular rules into text improvesR(r), but has little impact on other metrics,
including Acc(t).
Models Setting Airline Tax
AC(t) R( t) Acc( t) AC( t) R( t) Acc( t)
Llama 70B Table 0.764 0.558 0.01 0.834 0.989 0.01
Text 0.764 0.582 0.01 0.814 0.991 0.00
Qwen 72B Table 0.636 0.586 0.01 0.888 0.998 0.10
Text 0.748 0.633 0.02 0.859 0.996 0.01
Llama 405B Table 0.854 0.604 0.03 0.923 0.999 0.16
Text 0.835 0.587 0.07 0.919 0.998 0.05
Claude-3.5 Table 0.930 0.702 0.04 0.964 1.000 0.32
Text 0.937 0.705 0.06 0.971 1.000 0.33
GPT-4o Table 0.862 0.616 0.02 0.965 1.000 0.42
Text 0.864 0.669 0.03 0.960 1.000 0.33
Table 9: Results of different LLMs given different rule representations.
D.2.4 Do Distractive Rules Matter?
An essential aspect of rule-following involves identifying which rules are relevant to the current problem.
In our experiments, all domain-specific rules are provided in the prompt, leaving it to the LLMs to
determine which ones should be applied. To assess the extent to which irrelevant rules detrimentally affect
performance, we focus on the tax scenario. In this domain, we can introduce additional tax forms that
contain only zero values, effectively rendering any corresponding rules irrelevant. Despite these rules
being unnecessary, their mere presence may mislead LLMs into treating them as important.
To isolate the effect of these distractive rules from the influence of increased context length, we also
create a “Placeholder” setting. In this setting, we replace the distractive rules with an equivalent amount
of meaningless tokens that do not correspond to any rules. By comparing performance under these two
conditions, we can distinguish between the impact of irrelevant rules and the general challenge posed by a
longer input.
As shown in Figure 8, the presence of distractive (irrelevant) rules significantly degrades LLM perfor-
mance, while increasing context length using meaningless placeholders results in only a minor perfor-
mance drop. These findings suggest that LLMs remain vulnerable to distraction, which undermines their
reliability when confronted with superfluous, yet superficially valid, rules.
570

Llama-3.1 70BQwen-2.5 72BLlama-3.1 405BClaude-3.5 Sonnet
GPT-4o
0.6
0.7
0.8
0.9
1.0PC
Standard
Distractor
Placeholder
(a) Correctness
Llama-3.1 70BQwen-2.5 72BLlama-3.1 405BClaude-3.5 Sonnet
GPT-4o
0.6
0.7
0.8
0.9
1.0PR (b) Recall
Llama-3.1 70BQwen-2.5 72BLlama-3.1 405BClaude-3.5 Sonnet
GPT-4o
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7Acc (c) Accuracy
Figure 8: The effect of distractive rules and context length. The “Standard” mode refers to the default setting of
Level 1 tax problems, the “Distractor” mode appends nullified forms after the “Standard” input, and the “Placeholder”
mode adds meaningless tokens on space lines. Distractive rules lead to a significant drop on the performances of all
LLMs, while meaningless tokens make little difference to the performance.
D.2.5 Can Tool Augmentation Help?
In Appendix D.2.1, we notice that only near-perfect application correctness AC(t) can lead to significant
accuracy improvement. As the most simple way to reduce errors, especially in mathematical and logical
operations, is to introduce external tools, we wonder to what extent external tools can help in our
rule-guided reasoning tasks.
Following program of thoughts (Chen et al., 2023) prompting, we ask our LLMs to write Python code
to calculate the answer on airline bag fee tasks by defining a solution() function and returning the
total_cost variable, and the execution result of solution() function is viewed as the predicted answer.
In this way, the Python interpreter can be viewed as an oracle tool for mathematical and logical calculation.
To ensure the correct format of response, we use the 1-shot setting, so we compare the additional results
with the original 1-shot results in Table 3 as follows:
Models Setting Level 1 Level 2
AC(t) R( t) Acc( t) AC( t) R( t) Acc( t)
Llama 70B 1-shot Default 0.809 0.787 0.17 0.827 0.801 0.07
Tool Augmented 0.863 0.882 0.34 0.827 0.887 0.18
Qwen 72B 1-shot Default 0.836 0.908 0.19 0.818 0.901 0.10
Tool Augmented 0.939 0.899 0.42 0.946 0.899 0.26
GPT-4o 1-shot Default 0.922 0.885 0.32 0.875 0.853 0.16
Tool Augmented 0.939 0.914 0.44 0.937 0.940 0.33
Table 10: Results of different LLMs with tool augmentation on airline tasks.
As can be seen from these results, when provided with oracle math and logic tools, LLMs can achieve
a significant performance boost in terms of accuracy Acc(t). However, even provided with such tools,
LLMs are far from being able to resolve our rule-guided reasoning tasks. We observe non-perfect recall
R(t) and correctness AC(t), which indicates that LLMs still make mistakes in generated codes.
D.2.6 Summary of Factors that Influence Rule-Guided Following
In summary, various factors, such as rule complexity, the presence of distractive information, and
the difficulty gap between in-context examples and target problems, can profoundly influence LLM
performance. Even when LLMs succeed in simpler conditions, challenges like complex mathematical
reasoning, large amounts of extraneous rules, and non-ideal in-context samples can severely limit their
effectiveness on RULE ARENA problems.
571

E LLM Prompts
Airline. The prompt template we use in airline domain is as follows.
System Prompt : You are a helpful assistant at American Airlines .
User Prompt : You are given the information of a passenger , his / her items , his / her special needs , and the
policies of American Airlines . You should compute the total cost ( including the flight ticket fee , checked
bag fees , cost of special needs ) according to the policies for the passenger . The policies of American
Airlines are as follows :
< reference_rules >
< user_query > Compute the total cost for him step by step ( don ' t omit any bag ) and end your response with
" The total cost is $xxx ." ( xxx is a number )
Your response :
NBA. The prompt template we use in NBA domain is as follows.
System Prompt : You are a helpful NBA team consultant .
User Prompt : You are given rules in NBA Collective Bargaining Agreement and the information about some teams
and players . Then you will be given a list of operations , each of which desribes how some teams conduct
some transaction . You should determine whether each operation complies with the given rules .
Assume :
* the Salary Cap for the prior (2023 -24) Salary Cap Year is $136 ,000 ,000;
* the Average Player Salary for the prior (2023 -24) Salary Cap Year is $9 ,700 ,000;
* the Salary Cap for the current (2024 -25) NBA Salary Cap Year is $140 ,588 ,000;
* the Luxury Tax is $170 ,814 ,000;
* the First Apron Level is $178 ,132 ,000;
* the Second Apron Level is $188 ,931 ,000;
* the Team Salary of each team listed under " Team Situations :" do not include the amount of contracts that
expire at the end of 2023 -2024 Salary Cap Year .
Reference Rules in NBA Collective Bargaining Agreement :
< reference_rules >
Decide whether any operation by any team violate the rules :
< user_query >
Analyze the described operations and explicitly state the type of Salary Cap Exceptions if you think the
exception should be involved . Conclude your response with :
* " Answer : False ." if there is no violation to the rules ;
* " Answer : True . Illegal Operation : X . Problematic Team : Y ." if Team Y in Operation X violates the rules .
Both X and Y should be a single capital letter as A / B / C /...
Your response :
Tax. The prompt template we use in tax domain is as follows, where “<irs_forms>” includes both form
instructions and user query information.
System Prompt : You are a helpful US taxation consultant .
User Prompt : You are given several forms used to report US income tax and the instructions or rules about how
to fill the forms . Then you will be given the income and / or payment information about a tax payer According
to the given information . You should calculate the income tax owed by this payer .
IRS Forms for the tax payer :
< irs_forms >
Calculate the tax owed by the payer step - by - step according to the information provided by the forms . You
should calculate all fields marked with [ __ ]. DO NOT round numbers without explicit instructions . End your
response with :
1. " The total tax owed is $xxx ." ( xxx is a number ) if there is tax owed .
2. " The total tax overpaid is $xxx ." ( xxx is a number ) if there is tax overpaid ( and should be refunded ) .
Your response :
572